{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "4\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from PreProcessing.Dictionary import sd, ad, kopro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Using cached konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kcy93\\appdata\\roaming\\python\\python38\\site-packages (from konlpy) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\kcy93\\anaconda3\\lib\\site-packages (from konlpy) (1.18.5)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\kcy93\\anaconda3\\lib\\site-packages (from konlpy) (4.5.2)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.0.2-cp38-cp38-win_amd64.whl (1.6 MB)\n",
      "Collecting beautifulsoup4==4.6.0\n",
      "  Using cached beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
      "Collecting tweepy>=3.7.0\n",
      "  Using cached tweepy-3.9.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\kcy93\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\kcy93\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (2.24.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\kcy93\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\kcy93\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\kcy93\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kcy93\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\kcy93\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: JPype1, beautifulsoup4, oauthlib, requests-oauthlib, tweepy, konlpy\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.9.1\n",
      "    Uninstalling beautifulsoup4-4.9.1:\n",
      "      Successfully uninstalled beautifulsoup4-4.9.1\n",
      "Successfully installed JPype1-1.0.2 beautifulsoup4-4.6.0 konlpy-0.5.2 oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim w2v\n",
    "- size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "- window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "- min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "- workers: (default 3) The number of threads to use while training.\n",
    "- sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모든 EBS 와 모든 google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STT1_Google(s, e):\n",
    "    # 파일 이름 만들기 #################################################\n",
    "    STT_filenames = []\n",
    "    for i in range(s, e+1):\n",
    "        n = '{0:05d}'.format(i)\n",
    "        STT_fname = 'lec' + n + '_STT1_Google.txt'\n",
    "        STT_filenames.append(STT_fname)\n",
    "    print(STT_filenames)\n",
    "    ####################################################\n",
    "    \n",
    "    # 파일 불러오기 ###################################################\n",
    "    STT_integrated = ''\n",
    "    \n",
    "    try:\n",
    "        for _ in STT_filenames:\n",
    "            with open('data/' + _, 'r', encoding = 'euc-kr') as f:\n",
    "                if STT_filenames.index(_) == 0:\n",
    "                    STT_integrated += f.read() \n",
    "                else:\n",
    "                    STT_integrated += ' '+f.read()\n",
    "    except:\n",
    "        for _ in STT_filenames:\n",
    "            with open('data/' + _, 'r') as f:\n",
    "                if STT_filenames.index(_) == 0:\n",
    "                    STT_integrated += f.read() \n",
    "                else:\n",
    "                    STT_integrated += ' '+f.read()   \n",
    "    ####################################################\n",
    "    \n",
    "    return STT_integrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lec00001_EBS.txt', 'lec00002_EBS.txt', 'lec00003_EBS.txt', 'lec00004_EBS.txt', 'lec00005_EBS.txt', 'lec00006_EBS.txt', 'lec00007_EBS.txt', 'lec00008_EBS.txt', 'lec00009_EBS.txt', 'lec00010_EBS.txt', 'lec00011_EBS.txt', 'lec00012_EBS.txt', 'lec00013_EBS.txt', 'lec00014_EBS.txt', 'lec00015_EBS.txt', 'lec00016_EBS.txt', 'lec00017_EBS.txt', 'lec00018_EBS.txt', 'lec00019_EBS.txt', 'lec00020_EBS.txt', 'lec00021_EBS.txt', 'lec00022_EBS.txt', 'lec00023_EBS.txt', 'lec00024_EBS.txt', 'lec00025_EBS.txt', 'lec00026_EBS.txt']\n"
     ]
    }
   ],
   "source": [
    "from PreProcessing.Opentext import get_EBS\n",
    "from PreProcessing.Opentext import get_STT1_Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimalnumber_to_korean(m):\n",
    "    answer = ''\n",
    "    for _ in m.group(1):\n",
    "        answer += kopro[int(_)]\n",
    "    answer += '점'\n",
    "    for _ in m.group(3):\n",
    "        answer += kopro[int(_)]\n",
    "    return answer\n",
    "\n",
    "def number_to_korean(s):\n",
    "    #소수점 한국어로\n",
    "    s = re.sub(r'(\\d+)(\\.)(\\d+)', decimalnumber_to_korean, s)\n",
    "    \n",
    "    #정수 한국어로\n",
    "    fl = re.findall(r'\\d+', s)\n",
    "    fl = set(fl)\n",
    "    fl = [int(_) for _ in fl]\n",
    "    fl.sort(reverse=True)\n",
    "    fl = [str(_) for _ in fl]\n",
    "    for n in fl:\n",
    "        s = s.replace(n, kopro[int(n)])\n",
    "    return s\n",
    "\n",
    "def mathsymbol_to_korean(s):\n",
    "    for ms in sd.keys():\n",
    "        if ms in s:\n",
    "            s = s.replace(ms, sd[ms])\n",
    "    return s\n",
    "\n",
    "def alphabet_to_korean(s):\n",
    "    for a in ad.keys():\n",
    "        if a in s:\n",
    "            s = s.replace(a, ad[a])\n",
    "    return s\n",
    "\n",
    "def remove_newlines(s):\n",
    "    \n",
    "    return re.sub(r'[\\n]+', ' ', s)\n",
    "\n",
    "def remove_puncuations(s):\n",
    "    \n",
    "    return re.sub(r'[\\.?,‘’]+', ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EBS 전처리\n",
    "def EBS_preprocessing(EBS):\n",
    "\n",
    "    \n",
    "#     수학기호 한국어로 변환(EBS)\n",
    "    ebs_math_conversion = mathsymbol_to_korean(EBS)\n",
    "        \n",
    "#     숫자 한국어로 변환(EBS)\n",
    "    ebs_number_conversion = number_to_korean(ebs_math_conversion)\n",
    "        \n",
    "#     알파벳 한국어로 변환(EBS)\n",
    "    ebs_alphabet_conversion = alphabet_to_korean(ebs_number_conversion)\n",
    "    \n",
    "#     특수문자 제거\n",
    "    ebs_punc_removed = remove_puncuations(ebs_alphabet_conversion)\n",
    "    \n",
    "#     점 집어넣기\n",
    "    #ebs_dot_inserted = ' '.join(insert_dot(ebs_punc_removed,'.'))\n",
    "\n",
    "#     띄어쓰기 제거\n",
    "    #ebs_whitespace_removed = re.sub('[\\s]+', '', ebs_dot_inserted)\n",
    "\n",
    "    #ebs_sep_by_lec = ebs_dot_inserted.replace('  ',' ')\n",
    "    return ebs_punc_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lec00001_EBS.txt', 'lec00002_EBS.txt', 'lec00003_EBS.txt', 'lec00004_EBS.txt', 'lec00005_EBS.txt', 'lec00006_EBS.txt', 'lec00007_EBS.txt', 'lec00008_EBS.txt', 'lec00009_EBS.txt', 'lec00010_EBS.txt', 'lec00011_EBS.txt', 'lec00012_EBS.txt', 'lec00013_EBS.txt', 'lec00014_EBS.txt', 'lec00015_EBS.txt', 'lec00016_EBS.txt', 'lec00017_EBS.txt', 'lec00018_EBS.txt', 'lec00019_EBS.txt', 'lec00020_EBS.txt', 'lec00021_EBS.txt', 'lec00022_EBS.txt', 'lec00023_EBS.txt', 'lec00024_EBS.txt', 'lec00025_EBS.txt', 'lec00026_EBS.txt']\n"
     ]
    }
   ],
   "source": [
    "new_EBS = EBS_preprocessing(get_EBS(1,26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGLE 전처리\n",
    "def GOOGLE_preprocessing(GOOGLE):\n",
    "        \n",
    "#     개행 삭제\n",
    "    google_newline_removed = remove_newlines(GOOGLE)\n",
    "    \n",
    "#     수학기호 한국어로 변환\n",
    "    google_math_conversion = mathsymbol_to_korean(google_newline_removed)\n",
    "    \n",
    "#     숫자 한국어로 변환(GOOGLE)\n",
    "    google_number_conversion = number_to_korean(google_math_conversion)\n",
    "        \n",
    "#     알파벳 한국어로 변환(GOOGLE)\n",
    "    google_alphabet_conversion = alphabet_to_korean(google_number_conversion)\n",
    "    \n",
    "#     특수문자 제거\n",
    "    google_punc_removed = remove_puncuations(google_alphabet_conversion)\n",
    "    \n",
    "#     점 집어넣기\n",
    "    #google_dot_inserted = ' '.join(insert_dot(google_punc_removed,'.'))\n",
    "    \n",
    "#     띄어쓰기 제거\n",
    "    #google_whitespace_removed = re.sub('[\\s]+', '', google_dot_inserted)\n",
    "\n",
    "    return google_punc_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lec00001_STT1_Google.txt', 'lec00002_STT1_Google.txt', 'lec00003_STT1_Google.txt', 'lec00004_STT1_Google.txt', 'lec00005_STT1_Google.txt', 'lec00006_STT1_Google.txt', 'lec00007_STT1_Google.txt', 'lec00008_STT1_Google.txt', 'lec00009_STT1_Google.txt', 'lec00010_STT1_Google.txt', 'lec00011_STT1_Google.txt', 'lec00012_STT1_Google.txt', 'lec00013_STT1_Google.txt', 'lec00014_STT1_Google.txt', 'lec00015_STT1_Google.txt', 'lec00016_STT1_Google.txt', 'lec00017_STT1_Google.txt', 'lec00018_STT1_Google.txt', 'lec00019_STT1_Google.txt', 'lec00020_STT1_Google.txt', 'lec00021_STT1_Google.txt', 'lec00022_STT1_Google.txt', 'lec00023_STT1_Google.txt', 'lec00024_STT1_Google.txt', 'lec00025_STT1_Google.txt', 'lec00026_STT1_Google.txt']\n"
     ]
    }
   ],
   "source": [
    "new_GOOGLE = get_STT1_Google(1,26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'반갑습니다 이비에스 여러분들 여러분들과 함께 중학뉴런수학 3할을 강의하기 됨 손성민입니다 '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_GOOGLE[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebs_pos = kkma.pos(new_EBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_google' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-eda65c2795d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgoogle_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkkma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_google\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'new_google' is not defined"
     ]
    }
   ],
   "source": [
    "google_pos = kkma.pos(new_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_pos(pos_text):\n",
    "    alist = []\n",
    "    for _ in pos_text:\n",
    "        if _[1] in ['NNG','NNP','NNB','NR','NP', 'VV','VA','OL']:\n",
    "            alist.append(_[0])\n",
    "            continue\n",
    "    return alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_EBS_pos = selected_pos(ebs_pos)\n",
    "list_google_pos = selected_pos(google_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [list_EBS_pos,list_google_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim w2v\n",
    "#train model ,CBOW\n",
    "model = Word2Vec(\n",
    "    sentences, \n",
    "    size = 150,\n",
    "    min_count=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['분수'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similar_by_word('사인',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-73adf7b7ac43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model_skip = Word2Vec(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Word2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "model_skip = Word2Vec(\n",
    "    sentences, \n",
    "    size = 150,\n",
    "    min_count=1,\n",
    "    sg = 1\n",
    ")\n",
    "print(model_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_skip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b21df0a12475>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_skip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tan'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_skip' is not defined"
     ]
    }
   ],
   "source": [
    "model_skip.wv.similar_by_word('tan',topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
