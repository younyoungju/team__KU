{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"colab learning_index_rev-80.ipynb의 사본","provenance":[{"file_id":"1bWNN5RDtjWXk-nSpQe7qIldYHDU1e3ME","timestamp":1599315699563}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"5k8BNytXbn6K","colab_type":"text"},"source":["# colab용 자동화"]},{"cell_type":"code","metadata":{"id":"W7jWEOhEbn6K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1599312211494,"user_tz":-540,"elapsed":19938,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}},"outputId":"963b5ad0-5f71-489b-e349-0cb2960c67fd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NhuLN_5vbn6L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599315627085,"user_tz":-540,"elapsed":3373731,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}},"outputId":"658dcc4d-0f29-4fe9-f249-1a30ecc56db5"},"source":["import pickle\n","# from soynlp.hangle import levenshtein\n","# # from PreProcessing.find_common_part\n","# from konlpy.tag import *\n","# from PreProcessing import find_common_part\n","import numpy as np\n","from gensim.models import Word2Vec\n","from keras.layers import Embedding\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Dense, LSTM\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","# 불러올때    \n","with open('/content/drive/Shared drives/BigData/team__KU/data/result.txt', 'rb') as f:\n","    result = pickle.load(f)\n","\n","result_gensim_input = [_.split() for _ in result if _ != '']\n","result_tokenizer_input = [v for i, v in enumerate(result) if i%2 == 0 and v != '']\n","ebs_in_result_for_getting_max_len = [v for i, v in enumerate(result_gensim_input) if i%2 == 0 and v != []]\n","ebs_in_result_gensim_input = [v for i, v in enumerate(result_gensim_input) if i%2 == 0 and v != [] ]\n","google_in_result_gensim_input = [v for i, v in enumerate(result_gensim_input) if i%2 == 1 and v != []]\n","    \n","# Generate EBS string vectors matrix\n","ws = 1\n","es = 30\n","\n","model_cbow = Word2Vec(\n","                ebs_in_result_gensim_input, \n","                window = ws,\n","                size =es,\n","                min_count=1,\n","                workers = 10\n","                )\n","vocabs = list(model_cbow.wv.index2word)\n","embedding_matrix = np.zeros((len(vocabs), es))\n","for i, w in enumerate(vocabs):\n","    embedding_matrix[i] = model_cbow[w]\n","\n","#embedding_matrix 맨 위에 0벡터 추가\n","stacked_zero = np.zeros((1, es))\n","embedding_matrix = np.vstack((stacked_zero, embedding_matrix))\n","\n","t = Tokenizer()\n","t.fit_on_texts(result_tokenizer_input)\n","vocab_size = len(t.word_index) + 1\n","\n","#시퀀스 만들기\n","sequences = list()\n","\n","for line in result_tokenizer_input: # 1,214 개의 샘플에 대해서 샘플을 1개씩 가져온다.\n","    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩\n","    for i in range(1, len(encoded)):\n","        sequence = encoded[i-9 if i > 9 else 0:i+1]\n","        sequences.append(sequence)\n","        \n","max_len=max(len(l) for l in sequences)\n","print('max length : {}'.format(max_len))\n","\n","sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n","\n","sequences = np.array(sequences)\n","X = sequences[:,:-1]\n","y = sequences[:,-1]\n","\n","y = to_categorical(y, num_classes=vocab_size)\n","\n","embedding_layer = Embedding(vocab_size,\n","                            es,\n","                            weights=[embedding_matrix],\n","                            input_length=max_len,\n","                            trainable=False)\n","\n","model = Sequential()\n","model.add(embedding_layer)\n","# y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1\n","model.add(LSTM(128, activation = 'relu'))\n","model.add(Dense(vocab_size, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X, y, epochs=80, verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"},{"output_type":"stream","text":["max length : 10\n","Epoch 1/80\n","WARNING:tensorflow:Model was constructed with shape (None, 10) for input Tensor(\"embedding_input:0\", shape=(None, 10), dtype=float32), but it was called on an input with incompatible shape (None, 9).\n","WARNING:tensorflow:Model was constructed with shape (None, 10) for input Tensor(\"embedding_input:0\", shape=(None, 10), dtype=float32), but it was called on an input with incompatible shape (None, 9).\n","1558/1558 - 43s - loss: 7.7881 - accuracy: 0.0161\n","Epoch 2/80\n","1558/1558 - 41s - loss: 7.4310 - accuracy: 0.0197\n","Epoch 3/80\n","1558/1558 - 42s - loss: 7.2676 - accuracy: 0.0235\n","Epoch 4/80\n","1558/1558 - 42s - loss: 7.1253 - accuracy: 0.0251\n","Epoch 5/80\n","1558/1558 - 41s - loss: 6.9884 - accuracy: 0.0273\n","Epoch 6/80\n","1558/1558 - 41s - loss: 6.8666 - accuracy: 0.0290\n","Epoch 7/80\n","1558/1558 - 42s - loss: 6.7507 - accuracy: 0.0313\n","Epoch 8/80\n","1558/1558 - 42s - loss: 6.6188 - accuracy: 0.0339\n","Epoch 9/80\n","1558/1558 - 42s - loss: 6.4699 - accuracy: 0.0357\n","Epoch 10/80\n","1558/1558 - 41s - loss: 6.3057 - accuracy: 0.0377\n","Epoch 11/80\n","1558/1558 - 42s - loss: 6.1357 - accuracy: 0.0399\n","Epoch 12/80\n","1558/1558 - 42s - loss: 5.9577 - accuracy: 0.0404\n","Epoch 13/80\n","1558/1558 - 42s - loss: 5.7587 - accuracy: 0.0425\n","Epoch 14/80\n","1558/1558 - 41s - loss: 5.5339 - accuracy: 0.0462\n","Epoch 15/80\n","1558/1558 - 42s - loss: 5.2997 - accuracy: 0.0670\n","Epoch 16/80\n","1558/1558 - 43s - loss: 5.0797 - accuracy: 0.1024\n","Epoch 17/80\n","1558/1558 - 42s - loss: 4.8899 - accuracy: 0.1359\n","Epoch 18/80\n","1558/1558 - 42s - loss: 4.7264 - accuracy: 0.1650\n","Epoch 19/80\n","1558/1558 - 42s - loss: 4.5833 - accuracy: 0.1866\n","Epoch 20/80\n","1558/1558 - 41s - loss: 4.4641 - accuracy: 0.2040\n","Epoch 21/80\n","1558/1558 - 42s - loss: 4.3509 - accuracy: 0.2196\n","Epoch 22/80\n","1558/1558 - 42s - loss: 4.2572 - accuracy: 0.2301\n","Epoch 23/80\n","1558/1558 - 42s - loss: 4.1681 - accuracy: 0.2409\n","Epoch 24/80\n","1558/1558 - 42s - loss: 4.0913 - accuracy: 0.2525\n","Epoch 25/80\n","1558/1558 - 42s - loss: 4.0151 - accuracy: 0.2611\n","Epoch 26/80\n","1558/1558 - 42s - loss: 3.9500 - accuracy: 0.2668\n","Epoch 27/80\n","1558/1558 - 42s - loss: 3.8838 - accuracy: 0.2755\n","Epoch 28/80\n","1558/1558 - 42s - loss: 3.8238 - accuracy: 0.2820\n","Epoch 29/80\n","1558/1558 - 41s - loss: 3.7633 - accuracy: 0.2884\n","Epoch 30/80\n","1558/1558 - 41s - loss: 3.7130 - accuracy: 0.2960\n","Epoch 31/80\n","1558/1558 - 42s - loss: 3.6609 - accuracy: 0.3031\n","Epoch 32/80\n","1558/1558 - 42s - loss: 3.6166 - accuracy: 0.3065\n","Epoch 33/80\n","1558/1558 - 42s - loss: 3.5684 - accuracy: 0.3153\n","Epoch 34/80\n","1558/1558 - 42s - loss: 3.5227 - accuracy: 0.3195\n","Epoch 35/80\n","1558/1558 - 42s - loss: 3.4817 - accuracy: 0.3244\n","Epoch 36/80\n","1558/1558 - 42s - loss: 3.4425 - accuracy: 0.3300\n","Epoch 37/80\n","1558/1558 - 42s - loss: 3.4004 - accuracy: 0.3367\n","Epoch 38/80\n","1558/1558 - 42s - loss: 3.3675 - accuracy: 0.3404\n","Epoch 39/80\n","1558/1558 - 43s - loss: 3.3277 - accuracy: 0.3457\n","Epoch 40/80\n","1558/1558 - 42s - loss: 3.2945 - accuracy: 0.3506\n","Epoch 41/80\n","1558/1558 - 41s - loss: 3.2581 - accuracy: 0.3550\n","Epoch 42/80\n","1558/1558 - 41s - loss: 3.2257 - accuracy: 0.3589\n","Epoch 43/80\n","1558/1558 - 41s - loss: 3.1919 - accuracy: 0.3637\n","Epoch 44/80\n","1558/1558 - 42s - loss: 3.1639 - accuracy: 0.3679\n","Epoch 45/80\n","1558/1558 - 42s - loss: 3.1308 - accuracy: 0.3728\n","Epoch 46/80\n","1558/1558 - 43s - loss: 3.1034 - accuracy: 0.3766\n","Epoch 47/80\n","1558/1558 - 42s - loss: 3.0726 - accuracy: 0.3796\n","Epoch 48/80\n","1558/1558 - 42s - loss: 3.0496 - accuracy: 0.3846\n","Epoch 49/80\n","1558/1558 - 42s - loss: 3.0221 - accuracy: 0.3884\n","Epoch 50/80\n","1558/1558 - 42s - loss: 2.9955 - accuracy: 0.3916\n","Epoch 51/80\n","1558/1558 - 42s - loss: 2.9728 - accuracy: 0.3936\n","Epoch 52/80\n","1558/1558 - 42s - loss: 2.9488 - accuracy: 0.3985\n","Epoch 53/80\n","1558/1558 - 42s - loss: 2.9267 - accuracy: 0.4007\n","Epoch 54/80\n","1558/1558 - 42s - loss: 2.9006 - accuracy: 0.4057\n","Epoch 55/80\n","1558/1558 - 43s - loss: 2.8849 - accuracy: 0.4068\n","Epoch 56/80\n","1558/1558 - 43s - loss: 2.8592 - accuracy: 0.4112\n","Epoch 57/80\n","1558/1558 - 42s - loss: 2.8451 - accuracy: 0.4136\n","Epoch 58/80\n","1558/1558 - 42s - loss: 2.8155 - accuracy: 0.4182\n","Epoch 59/80\n","1558/1558 - 42s - loss: 2.8027 - accuracy: 0.4208\n","Epoch 60/80\n","1558/1558 - 42s - loss: 2.7796 - accuracy: 0.4233\n","Epoch 61/80\n","1558/1558 - 42s - loss: 2.7586 - accuracy: 0.4255\n","Epoch 62/80\n","1558/1558 - 42s - loss: 2.7381 - accuracy: 0.4318\n","Epoch 63/80\n","1558/1558 - 41s - loss: 2.7218 - accuracy: 0.4323\n","Epoch 64/80\n","1558/1558 - 42s - loss: 2.7053 - accuracy: 0.4352\n","Epoch 65/80\n","1558/1558 - 42s - loss: 2.6878 - accuracy: 0.4396\n","Epoch 66/80\n","1558/1558 - 43s - loss: 2.6717 - accuracy: 0.4395\n","Epoch 67/80\n","1558/1558 - 43s - loss: 2.6560 - accuracy: 0.4436\n","Epoch 68/80\n","1558/1558 - 42s - loss: 2.6371 - accuracy: 0.4440\n","Epoch 69/80\n","1558/1558 - 43s - loss: 2.6238 - accuracy: 0.4480\n","Epoch 70/80\n","1558/1558 - 42s - loss: 2.6097 - accuracy: 0.4487\n","Epoch 71/80\n","1558/1558 - 42s - loss: 2.5922 - accuracy: 0.4536\n","Epoch 72/80\n","1558/1558 - 42s - loss: 2.5783 - accuracy: 0.4536\n","Epoch 73/80\n","1558/1558 - 42s - loss: 2.5640 - accuracy: 0.4556\n","Epoch 74/80\n","1558/1558 - 42s - loss: 2.5501 - accuracy: 0.4576\n","Epoch 75/80\n","1558/1558 - 42s - loss: 2.5371 - accuracy: 0.4605\n","Epoch 76/80\n","1558/1558 - 44s - loss: 2.5274 - accuracy: 0.4628\n","Epoch 77/80\n","1558/1558 - 42s - loss: 2.5069 - accuracy: 0.4661\n","Epoch 78/80\n","1558/1558 - 42s - loss: 2.4978 - accuracy: 0.4666\n","Epoch 79/80\n","1558/1558 - 42s - loss: 2.4791 - accuracy: 0.4698\n","Epoch 80/80\n","1558/1558 - 42s - loss: 2.4702 - accuracy: 0.4726\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fa32b4e2550>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"Sw2CcHElbn6M","colab_type":"text"},"source":["# --------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"aZIK9svVbn6N","colab_type":"code","colab":{}},"source":["# for _ in list(range(10,160,10)):\n","model_skip = Word2Vec(\n","result_gensim_input, \n","window = 2,\n","size =100,\n","min_count=1,\n","workers = 10\n",")\n","print(_)\n","print('집값삼각형 : ', model_skip.most_similar('집값삼각형',topn=5))\n","print()\n","print('도영 : ', model_skip.most_similar('도영',topn=5))\n","print()\n","print('싸인 : ', model_skip.most_similar('싸인',topn=5))\n","print()\n","print('수박 : ',model_skip.most_similar('수박',topn=5)) \n","print()\n","print('가게대변인 : ',model_skip.most_similar('가게대변인',topn=5))\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fl_PL5Xabn6O","colab_type":"code","colab":{}},"source":["# model_skip = Word2Vec(\n","#     result_gensim_input, \n","#     window = 2,\n","#     size =40,\n","#     min_count=1,\n","#     sg = 1,\n","#     workers = 10\n","# )\n","# print(model_skip)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1tEEKB_zbn6P","colab_type":"code","colab":{}},"source":["model_skip.most_similar('집값삼각형',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m_DFp-lEbn6R","colab_type":"code","colab":{}},"source":["model_skip.most_similar('도영',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yUBh1M_Sbn6S","colab_type":"code","colab":{}},"source":["model_skip.most_similar('싸인',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltnOo52qbn6T","colab_type":"code","colab":{}},"source":["model_skip.most_similar('수박',topn=10) #특수각"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-umUhNxbn6U","colab_type":"code","colab":{}},"source":["model_skip.most_similar('가게대변인',topn=10) #각의대변"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5o4NWTybn6W","colab_type":"code","colab":{}},"source":["model_skip.most_similar('추위',topn=10) #최애"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Az-lalAJbn6Z","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLn_Be83bn6b","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vr6-9POdbn6c","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xukcDftEbn6e","colab_type":"code","colab":{}},"source":["model_skip.most_similar('',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-0P0HUsbn6f","colab_type":"code","colab":{}},"source":["model_skip.most_similar('',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_m1xQ9tbn6g","colab_type":"code","colab":{}},"source":["model_skip.most_similar('',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzbq8ymebn6h","colab_type":"code","colab":{}},"source":["model_skip.most_similar('',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxU8khcebn6i","colab_type":"code","colab":{}},"source":["model_skip.most_similar('',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfvh23dcbn6j","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqNNlXkqbn6k","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mIAjJLEMbn6l","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Lg00H6qbn6m","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LipRlvYbn6n","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}