{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"colab learning_index_rev-100.ipynb의 사본","provenance":[{"file_id":"1MR5GPOXLaQWRxrmHrAqE8r-Dca0iPzc5","timestamp":1599317343389}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"oLIIdjcGdXWh","colab_type":"text"},"source":["# colab용 자동화"]},{"cell_type":"code","metadata":{"id":"DzM03QsrdXWh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1599312642764,"user_tz":-540,"elapsed":16494,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}},"outputId":"a9452203-f880-4d9d-ebe0-8c1512571b1d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XJ_3w8f2dXWk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599316850723,"user_tz":-540,"elapsed":4194033,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}},"outputId":"b152e947-179a-4714-ab73-0e6e29c53490"},"source":["import pickle\n","# from soynlp.hangle import levenshtein\n","# # from PreProcessing.find_common_part\n","# from konlpy.tag import *\n","# from PreProcessing import find_common_part\n","import numpy as np\n","from gensim.models import Word2Vec\n","from keras.layers import Embedding\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Dense, LSTM\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","# 불러올때    \n","with open('/content/drive/Shared drives/BigData/team__KU/data/result.txt', 'rb') as f:\n","    result = pickle.load(f)\n","\n","result_gensim_input = [_.split() for _ in result if _ != '']\n","result_tokenizer_input = [v for i, v in enumerate(result) if i%2 == 0 and v != '']\n","ebs_in_result_for_getting_max_len = [v for i, v in enumerate(result_gensim_input) if i%2 == 0 and v != []]\n","ebs_in_result_gensim_input = [v for i, v in enumerate(result_gensim_input) if i%2 == 0 and v != [] ]\n","google_in_result_gensim_input = [v for i, v in enumerate(result_gensim_input) if i%2 == 1 and v != []]\n","    \n","# Generate EBS string vectors matrix\n","ws = 1\n","es = 30\n","\n","model_cbow = Word2Vec(\n","                ebs_in_result_gensim_input, \n","                window = ws,\n","                size =es,\n","                min_count=1,\n","                workers = 10\n","                )\n","vocabs = list(model_cbow.wv.index2word)\n","embedding_matrix = np.zeros((len(vocabs), es))\n","for i, w in enumerate(vocabs):\n","    embedding_matrix[i] = model_cbow[w]\n","\n","#embedding_matrix 맨 위에 0벡터 추가\n","stacked_zero = np.zeros((1, es))\n","embedding_matrix = np.vstack((stacked_zero, embedding_matrix))\n","\n","t = Tokenizer()\n","t.fit_on_texts(result_tokenizer_input)\n","vocab_size = len(t.word_index) + 1\n","\n","#시퀀스 만들기\n","sequences = list()\n","\n","for line in result_tokenizer_input: # 1,214 개의 샘플에 대해서 샘플을 1개씩 가져온다.\n","    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩\n","    for i in range(1, len(encoded)):\n","        sequence = encoded[i-9 if i > 9 else 0:i+1]\n","        sequences.append(sequence)\n","        \n","max_len=max(len(l) for l in sequences)\n","print('max length : {}'.format(max_len))\n","\n","sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n","\n","sequences = np.array(sequences)\n","X = sequences[:,:-1]\n","y = sequences[:,-1]\n","\n","y = to_categorical(y, num_classes=vocab_size)\n","\n","embedding_layer = Embedding(vocab_size,\n","                            es,\n","                            weights=[embedding_matrix],\n","                            input_length=max_len,\n","                            trainable=False)\n","\n","model = Sequential()\n","model.add(embedding_layer)\n","# y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1\n","model.add(LSTM(128, activation = 'relu'))\n","model.add(Dense(vocab_size, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X, y, epochs=100, verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"},{"output_type":"stream","text":["max length : 10\n","Epoch 1/100\n","WARNING:tensorflow:Model was constructed with shape (None, 10) for input Tensor(\"embedding_input:0\", shape=(None, 10), dtype=float32), but it was called on an input with incompatible shape (None, 9).\n","WARNING:tensorflow:Model was constructed with shape (None, 10) for input Tensor(\"embedding_input:0\", shape=(None, 10), dtype=float32), but it was called on an input with incompatible shape (None, 9).\n","1558/1558 - 41s - loss: 7.7683 - accuracy: 0.0166\n","Epoch 2/100\n","1558/1558 - 40s - loss: 7.4193 - accuracy: 0.0203\n","Epoch 3/100\n","1558/1558 - 40s - loss: 7.2618 - accuracy: 0.0224\n","Epoch 4/100\n","1558/1558 - 41s - loss: 7.1348 - accuracy: 0.0260\n","Epoch 5/100\n","1558/1558 - 41s - loss: 7.0146 - accuracy: 0.0264\n","Epoch 6/100\n","1558/1558 - 41s - loss: 6.8980 - accuracy: 0.0286\n","Epoch 7/100\n","1558/1558 - 41s - loss: 6.7833 - accuracy: 0.0306\n","Epoch 8/100\n","1558/1558 - 42s - loss: 6.6532 - accuracy: 0.0324\n","Epoch 9/100\n","1558/1558 - 41s - loss: 6.5127 - accuracy: 0.0340\n","Epoch 10/100\n","1558/1558 - 41s - loss: 6.3689 - accuracy: 0.0350\n","Epoch 11/100\n","1558/1558 - 41s - loss: 6.2121 - accuracy: 0.0367\n","Epoch 12/100\n","1558/1558 - 41s - loss: 6.0446 - accuracy: 0.0385\n","Epoch 13/100\n","1558/1558 - 42s - loss: 5.8523 - accuracy: 0.0397\n","Epoch 14/100\n","1558/1558 - 41s - loss: 5.6394 - accuracy: 0.0462\n","Epoch 15/100\n","1558/1558 - 41s - loss: 5.4203 - accuracy: 0.0623\n","Epoch 16/100\n","1558/1558 - 41s - loss: 5.2098 - accuracy: 0.0952\n","Epoch 17/100\n","1558/1558 - 41s - loss: 5.0159 - accuracy: 0.1293\n","Epoch 18/100\n","1558/1558 - 41s - loss: 4.8498 - accuracy: 0.1578\n","Epoch 19/100\n","1558/1558 - 41s - loss: 4.7042 - accuracy: 0.1810\n","Epoch 20/100\n","1558/1558 - 41s - loss: 4.5774 - accuracy: 0.1978\n","Epoch 21/100\n","1558/1558 - 42s - loss: 4.4668 - accuracy: 0.2123\n","Epoch 22/100\n","1558/1558 - 41s - loss: 4.3680 - accuracy: 0.2228\n","Epoch 23/100\n","1558/1558 - 42s - loss: 4.2791 - accuracy: 0.2337\n","Epoch 24/100\n","1558/1558 - 41s - loss: 4.1959 - accuracy: 0.2428\n","Epoch 25/100\n","1558/1558 - 41s - loss: 4.1195 - accuracy: 0.2514\n","Epoch 26/100\n","1558/1558 - 41s - loss: 4.0521 - accuracy: 0.2589\n","Epoch 27/100\n","1558/1558 - 41s - loss: 3.9815 - accuracy: 0.2674\n","Epoch 28/100\n","1558/1558 - 41s - loss: 3.9177 - accuracy: 0.2747\n","Epoch 29/100\n","1558/1558 - 42s - loss: 3.8619 - accuracy: 0.2809\n","Epoch 30/100\n","1558/1558 - 41s - loss: 3.8015 - accuracy: 0.2886\n","Epoch 31/100\n","1558/1558 - 41s - loss: 3.7495 - accuracy: 0.2933\n","Epoch 32/100\n","1558/1558 - 41s - loss: 3.7018 - accuracy: 0.2999\n","Epoch 33/100\n","1558/1558 - 41s - loss: 3.6512 - accuracy: 0.3065\n","Epoch 34/100\n","1558/1558 - 41s - loss: 3.6076 - accuracy: 0.3105\n","Epoch 35/100\n","1558/1558 - 41s - loss: 3.5645 - accuracy: 0.3169\n","Epoch 36/100\n","1558/1558 - 42s - loss: 3.5251 - accuracy: 0.3228\n","Epoch 37/100\n","1558/1558 - 41s - loss: 3.4864 - accuracy: 0.3265\n","Epoch 38/100\n","1558/1558 - 41s - loss: 3.4477 - accuracy: 0.3316\n","Epoch 39/100\n","1558/1558 - 42s - loss: 3.4155 - accuracy: 0.3370\n","Epoch 40/100\n","1558/1558 - 42s - loss: 3.3797 - accuracy: 0.3415\n","Epoch 41/100\n","1558/1558 - 42s - loss: 3.3471 - accuracy: 0.3447\n","Epoch 42/100\n","1558/1558 - 42s - loss: 3.3175 - accuracy: 0.3495\n","Epoch 43/100\n","1558/1558 - 42s - loss: 3.2875 - accuracy: 0.3543\n","Epoch 44/100\n","1558/1558 - 43s - loss: 3.2566 - accuracy: 0.3577\n","Epoch 45/100\n","1558/1558 - 42s - loss: 3.2300 - accuracy: 0.3622\n","Epoch 46/100\n","1558/1558 - 42s - loss: 3.2006 - accuracy: 0.3657\n","Epoch 47/100\n","1558/1558 - 42s - loss: 3.1761 - accuracy: 0.3678\n","Epoch 48/100\n","1558/1558 - 42s - loss: 3.1499 - accuracy: 0.3705\n","Epoch 49/100\n","1558/1558 - 42s - loss: 3.1247 - accuracy: 0.3766\n","Epoch 50/100\n","1558/1558 - 42s - loss: 3.1042 - accuracy: 0.3782\n","Epoch 51/100\n","1558/1558 - 42s - loss: 3.0768 - accuracy: 0.3817\n","Epoch 52/100\n","1558/1558 - 43s - loss: 3.0585 - accuracy: 0.3859\n","Epoch 53/100\n","1558/1558 - 42s - loss: 3.0316 - accuracy: 0.3877\n","Epoch 54/100\n","1558/1558 - 42s - loss: 3.0142 - accuracy: 0.3910\n","Epoch 55/100\n","1558/1558 - 42s - loss: 2.9925 - accuracy: 0.3936\n","Epoch 56/100\n","1558/1558 - 42s - loss: 2.9724 - accuracy: 0.3967\n","Epoch 57/100\n","1558/1558 - 42s - loss: 2.9519 - accuracy: 0.4015\n","Epoch 58/100\n","1558/1558 - 42s - loss: 2.9380 - accuracy: 0.4018\n","Epoch 59/100\n","1558/1558 - 42s - loss: 2.9155 - accuracy: 0.4049\n","Epoch 60/100\n","1558/1558 - 42s - loss: 2.9007 - accuracy: 0.4068\n","Epoch 61/100\n","1558/1558 - 42s - loss: 2.8742 - accuracy: 0.4119\n","Epoch 62/100\n","1558/1558 - 42s - loss: 2.8633 - accuracy: 0.4140\n","Epoch 63/100\n","1558/1558 - 42s - loss: 2.8474 - accuracy: 0.4161\n","Epoch 64/100\n","1558/1558 - 42s - loss: 2.8302 - accuracy: 0.4161\n","Epoch 65/100\n","1558/1558 - 42s - loss: 2.8088 - accuracy: 0.4209\n","Epoch 66/100\n","1558/1558 - 43s - loss: 2.7947 - accuracy: 0.4231\n","Epoch 67/100\n","1558/1558 - 42s - loss: 2.7883 - accuracy: 0.4248\n","Epoch 68/100\n","1558/1558 - 42s - loss: 2.7676 - accuracy: 0.4260\n","Epoch 69/100\n","1558/1558 - 42s - loss: 2.7521 - accuracy: 0.4302\n","Epoch 70/100\n","1558/1558 - 42s - loss: 2.7368 - accuracy: 0.4323\n","Epoch 71/100\n","1558/1558 - 42s - loss: 2.7247 - accuracy: 0.4330\n","Epoch 72/100\n","1558/1558 - 42s - loss: 2.7069 - accuracy: 0.4370\n","Epoch 73/100\n","1558/1558 - 42s - loss: 2.7024 - accuracy: 0.4378\n","Epoch 74/100\n","1558/1558 - 43s - loss: 2.6904 - accuracy: 0.4389\n","Epoch 75/100\n","1558/1558 - 42s - loss: 2.6792 - accuracy: 0.4411\n","Epoch 76/100\n","1558/1558 - 42s - loss: 2.6581 - accuracy: 0.4446\n","Epoch 77/100\n","1558/1558 - 42s - loss: 2.6411 - accuracy: 0.4469\n","Epoch 78/100\n","1558/1558 - 42s - loss: 2.6383 - accuracy: 0.4465\n","Epoch 79/100\n","1558/1558 - 42s - loss: 2.6320 - accuracy: 0.4487\n","Epoch 80/100\n","1558/1558 - 42s - loss: 2.6195 - accuracy: 0.4496\n","Epoch 81/100\n","1558/1558 - 43s - loss: 2.5983 - accuracy: 0.4543\n","Epoch 82/100\n","1558/1558 - 42s - loss: 2.5969 - accuracy: 0.4525\n","Epoch 83/100\n","1558/1558 - 42s - loss: 2.5775 - accuracy: 0.4565\n","Epoch 84/100\n","1558/1558 - 42s - loss: 2.5733 - accuracy: 0.4567\n","Epoch 85/100\n","1558/1558 - 42s - loss: 2.5574 - accuracy: 0.4594\n","Epoch 86/100\n","1558/1558 - 42s - loss: 2.5470 - accuracy: 0.4609\n","Epoch 87/100\n","1558/1558 - 42s - loss: 2.5381 - accuracy: 0.4611\n","Epoch 88/100\n","1558/1558 - 42s - loss: 2.5273 - accuracy: 0.4641\n","Epoch 89/100\n","1558/1558 - 43s - loss: 2.5210 - accuracy: 0.4659\n","Epoch 90/100\n","1558/1558 - 42s - loss: 2.5088 - accuracy: 0.4668\n","Epoch 91/100\n","1558/1558 - 42s - loss: 2.4943 - accuracy: 0.4698\n","Epoch 92/100\n","1558/1558 - 42s - loss: 2.4941 - accuracy: 0.4692\n","Epoch 93/100\n","1558/1558 - 42s - loss: 2.4764 - accuracy: 0.4722\n","Epoch 94/100\n","1558/1558 - 42s - loss: 2.4739 - accuracy: 0.4726\n","Epoch 95/100\n","1558/1558 - 42s - loss: 2.4648 - accuracy: 0.4736\n","Epoch 96/100\n","1558/1558 - 42s - loss: 2.4531 - accuracy: 0.4762\n","Epoch 97/100\n","1558/1558 - 42s - loss: 2.4424 - accuracy: 0.4774\n","Epoch 98/100\n","1558/1558 - 42s - loss: 2.4365 - accuracy: 0.4780\n","Epoch 99/100\n","1558/1558 - 42s - loss: 2.4230 - accuracy: 0.4797\n","Epoch 100/100\n","1558/1558 - 42s - loss: 2.4145 - accuracy: 0.4825\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f846bf57710>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"PrnY-pmMdXWm","colab_type":"text"},"source":["# --------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"rMvQg6okdXWn","colab_type":"code","colab":{}},"source":["# for _ in list(range(10,160,10)):\n","model_skip = Word2Vec(\n","result_gensim_input, \n","window = 2,\n","size =100,\n","min_count=1,\n","workers = 10\n",")\n","print(_)\n","print('집값삼각형 : ', model_skip.most_similar('집값삼각형',topn=5))\n","print()\n","print('도영 : ', model_skip.most_similar('도영',topn=5))\n","print()\n","print('싸인 : ', model_skip.most_similar('싸인',topn=5))\n","print()\n","print('수박 : ',model_skip.most_similar('수박',topn=5)) \n","print()\n","print('가게대변인 : ',model_skip.most_similar('가게대변인',topn=5))\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSobn52YdXWp","colab_type":"code","colab":{}},"source":["# model_skip = Word2Vec(\n","#     result_gensim_input, \n","#     window = 2,\n","#     size =40,\n","#     min_count=1,\n","#     sg = 1,\n","#     workers = 10\n","# )\n","# print(model_skip)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86O-TVAEdXWs","colab_type":"code","colab":{}},"source":["model_skip.most_similar('집값삼각형',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OaSWfHfBdXWw","colab_type":"code","colab":{}},"source":["model_skip.most_similar('도영',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WwrD07SgdXWz","colab_type":"code","colab":{}},"source":["model_skip.most_similar('싸인',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tZJfx1YIdXW2","colab_type":"code","colab":{}},"source":["model_skip.most_similar('수박',topn=10) #특수각"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJA-curodXW4","colab_type":"code","colab":{}},"source":["model_skip.most_similar('가게대변인',topn=10) #각의대변"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWa1MH1ddXW6","colab_type":"code","colab":{}},"source":["model_skip.most_similar('추위',topn=10) #최애"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SetBxjVdXW-","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7n4TDY1GdXXC","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mI7dUTZdXXE","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fn-rJHgBdXXH","colab_type":"code","colab":{}},"source":["model_skip.most_similar('',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WV9zFOFdXXJ","colab_type":"code","colab":{}},"source":["model_skip.most_similar('',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BXX-4VndXXL","colab_type":"code","colab":{}},"source":["model_skip.most_similar('',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKvEsteZdXXN","colab_type":"code","colab":{}},"source":["model_skip.most_similar('',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntWcuP3qdXXO","colab_type":"code","colab":{}},"source":["model_skip.most_similar('',topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBLO8OU-dXXQ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1w1JfYPdXXS","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HIpbHFA8dXXU","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"heZGuysGdXXW","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HsOz880SdXXd","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}