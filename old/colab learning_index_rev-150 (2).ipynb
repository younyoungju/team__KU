{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DMthbL8em6P"
   },
   "source": [
    "# colab용 자동화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20165,
     "status": "ok",
     "timestamp": 1599312991451,
     "user": {
      "displayName": "이경찬",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64",
      "userId": "06819814280514699015"
     },
     "user_tz": -540
    },
    "id": "txf7ywX7em6P",
    "outputId": "60dc5215-ac9e-44d1-b914-66d462c5bcbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4923749,
     "status": "ok",
     "timestamp": 1599317934042,
     "user": {
      "displayName": "이경찬",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64",
      "userId": "06819814280514699015"
     },
     "user_tz": -540
    },
    "id": "qmyTcT_0em6Q",
    "outputId": "d06d1b20-1570-4e8e-867c-5ea4a86a4b43",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length : 10\n",
      "Epoch 1/150\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 10) for input Tensor(\"embedding_input:0\", shape=(None, 10), dtype=float32), but it was called on an input with incompatible shape (None, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 10) for input Tensor(\"embedding_input:0\", shape=(None, 10), dtype=float32), but it was called on an input with incompatible shape (None, 9).\n",
      "1558/1558 - 32s - loss: 7.7881 - accuracy: 0.0163\n",
      "Epoch 2/150\n",
      "1558/1558 - 33s - loss: 7.4081 - accuracy: 0.0197\n",
      "Epoch 3/150\n",
      "1558/1558 - 33s - loss: 7.2481 - accuracy: 0.0232\n",
      "Epoch 4/150\n",
      "1558/1558 - 33s - loss: 7.1150 - accuracy: 0.0249\n",
      "Epoch 5/150\n",
      "1558/1558 - 33s - loss: 7.0048 - accuracy: 0.0257\n",
      "Epoch 6/150\n",
      "1558/1558 - 34s - loss: 6.9041 - accuracy: 0.0267\n",
      "Epoch 7/150\n",
      "1558/1558 - 34s - loss: 6.7885 - accuracy: 0.0274\n",
      "Epoch 8/150\n",
      "1558/1558 - 32s - loss: 6.6598 - accuracy: 0.0285\n",
      "Epoch 9/150\n",
      "1558/1558 - 32s - loss: 6.5216 - accuracy: 0.0303\n",
      "Epoch 10/150\n",
      "1558/1558 - 32s - loss: 6.3677 - accuracy: 0.0319\n",
      "Epoch 11/150\n",
      "1558/1558 - 32s - loss: 6.1917 - accuracy: 0.0336\n",
      "Epoch 12/150\n",
      "1558/1558 - 32s - loss: 5.9846 - accuracy: 0.0355\n",
      "Epoch 13/150\n",
      "1558/1558 - 32s - loss: 5.7414 - accuracy: 0.0391\n",
      "Epoch 14/150\n",
      "1558/1558 - 32s - loss: 5.4740 - accuracy: 0.0542\n",
      "Epoch 15/150\n",
      "1558/1558 - 32s - loss: 5.2154 - accuracy: 0.0892\n",
      "Epoch 16/150\n",
      "1558/1558 - 32s - loss: 4.9864 - accuracy: 0.1276\n",
      "Epoch 17/150\n",
      "1558/1558 - 32s - loss: 4.7967 - accuracy: 0.1574\n",
      "Epoch 18/150\n",
      "1558/1558 - 32s - loss: 4.6392 - accuracy: 0.1794\n",
      "Epoch 19/150\n",
      "1558/1558 - 33s - loss: 4.5055 - accuracy: 0.1998\n",
      "Epoch 20/150\n",
      "1558/1558 - 33s - loss: 4.3861 - accuracy: 0.2118\n",
      "Epoch 21/150\n",
      "1558/1558 - 33s - loss: 4.2858 - accuracy: 0.2269\n",
      "Epoch 22/150\n",
      "1558/1558 - 32s - loss: 4.1917 - accuracy: 0.2376\n",
      "Epoch 23/150\n",
      "1558/1558 - 32s - loss: 4.1081 - accuracy: 0.2481\n",
      "Epoch 24/150\n",
      "1558/1558 - 32s - loss: 4.0309 - accuracy: 0.2567\n",
      "Epoch 25/150\n",
      "1558/1558 - 32s - loss: 3.9616 - accuracy: 0.2656\n",
      "Epoch 26/150\n",
      "1558/1558 - 33s - loss: 3.8920 - accuracy: 0.2744\n",
      "Epoch 27/150\n",
      "1558/1558 - 32s - loss: 3.8285 - accuracy: 0.2831\n",
      "Epoch 28/150\n",
      "1558/1558 - 32s - loss: 3.7660 - accuracy: 0.2898\n",
      "Epoch 29/150\n",
      "1558/1558 - 32s - loss: 3.7069 - accuracy: 0.2958\n",
      "Epoch 30/150\n",
      "1558/1558 - 32s - loss: 3.6515 - accuracy: 0.3033\n",
      "Epoch 31/150\n",
      "1558/1558 - 33s - loss: 3.6015 - accuracy: 0.3097\n",
      "Epoch 32/150\n",
      "1558/1558 - 33s - loss: 3.5526 - accuracy: 0.3164\n",
      "Epoch 33/150\n",
      "1558/1558 - 33s - loss: 3.5007 - accuracy: 0.3229\n",
      "Epoch 34/150\n",
      "1558/1558 - 33s - loss: 3.4584 - accuracy: 0.3283\n",
      "Epoch 35/150\n",
      "1558/1558 - 33s - loss: 3.4133 - accuracy: 0.3342\n",
      "Epoch 36/150\n",
      "1558/1558 - 34s - loss: 3.3700 - accuracy: 0.3395\n",
      "Epoch 37/150\n",
      "1558/1558 - 32s - loss: 3.3351 - accuracy: 0.3443\n",
      "Epoch 38/150\n",
      "1558/1558 - 32s - loss: 3.2940 - accuracy: 0.3508\n",
      "Epoch 39/150\n",
      "1558/1558 - 32s - loss: 3.2587 - accuracy: 0.3558\n",
      "Epoch 40/150\n",
      "1558/1558 - 32s - loss: 3.2259 - accuracy: 0.3600\n",
      "Epoch 41/150\n",
      "1558/1558 - 32s - loss: 3.1883 - accuracy: 0.3667\n",
      "Epoch 42/150\n",
      "1558/1558 - 32s - loss: 3.1562 - accuracy: 0.3713\n",
      "Epoch 43/150\n",
      "1558/1558 - 32s - loss: 3.1254 - accuracy: 0.3745\n",
      "Epoch 44/150\n",
      "1558/1558 - 32s - loss: 3.0957 - accuracy: 0.3775\n",
      "Epoch 45/150\n",
      "1558/1558 - 32s - loss: 3.0644 - accuracy: 0.3841\n",
      "Epoch 46/150\n",
      "1558/1558 - 33s - loss: 3.0359 - accuracy: 0.3877\n",
      "Epoch 47/150\n",
      "1558/1558 - 34s - loss: 3.0078 - accuracy: 0.3888\n",
      "Epoch 48/150\n",
      "1558/1558 - 34s - loss: 2.9806 - accuracy: 0.3966\n",
      "Epoch 49/150\n",
      "1558/1558 - 34s - loss: 2.9589 - accuracy: 0.4004\n",
      "Epoch 50/150\n",
      "1558/1558 - 33s - loss: 2.9336 - accuracy: 0.4029\n",
      "Epoch 51/150\n",
      "1558/1558 - 33s - loss: 2.9081 - accuracy: 0.4059\n",
      "Epoch 52/150\n",
      "1558/1558 - 32s - loss: 2.8793 - accuracy: 0.4097\n",
      "Epoch 53/150\n",
      "1558/1558 - 32s - loss: 2.8640 - accuracy: 0.4124\n",
      "Epoch 54/150\n",
      "1558/1558 - 32s - loss: 2.8395 - accuracy: 0.4163\n",
      "Epoch 55/150\n",
      "1558/1558 - 32s - loss: 2.8170 - accuracy: 0.4203\n",
      "Epoch 56/150\n",
      "1558/1558 - 32s - loss: 2.7946 - accuracy: 0.4239\n",
      "Epoch 57/150\n",
      "1558/1558 - 32s - loss: 2.7750 - accuracy: 0.4256\n",
      "Epoch 58/150\n",
      "1558/1558 - 32s - loss: 2.7501 - accuracy: 0.4293\n",
      "Epoch 59/150\n",
      "1558/1558 - 32s - loss: 2.7321 - accuracy: 0.4337\n",
      "Epoch 60/150\n",
      "1558/1558 - 32s - loss: 2.7134 - accuracy: 0.4359\n",
      "Epoch 61/150\n",
      "1558/1558 - 33s - loss: 2.6939 - accuracy: 0.4375\n",
      "Epoch 62/150\n",
      "1558/1558 - 33s - loss: 2.6738 - accuracy: 0.4431\n",
      "Epoch 63/150\n",
      "1558/1558 - 33s - loss: 2.6595 - accuracy: 0.4436\n",
      "Epoch 64/150\n",
      "1558/1558 - 33s - loss: 2.6419 - accuracy: 0.4477\n",
      "Epoch 65/150\n",
      "1558/1558 - 33s - loss: 2.6227 - accuracy: 0.4511\n",
      "Epoch 66/150\n",
      "1558/1558 - 33s - loss: 2.6028 - accuracy: 0.4539\n",
      "Epoch 67/150\n",
      "1558/1558 - 32s - loss: 2.5897 - accuracy: 0.4546\n",
      "Epoch 68/150\n",
      "1558/1558 - 32s - loss: 2.5766 - accuracy: 0.4561\n",
      "Epoch 69/150\n",
      "1558/1558 - 32s - loss: 2.5643 - accuracy: 0.4569\n",
      "Epoch 70/150\n",
      "1558/1558 - 32s - loss: 2.5433 - accuracy: 0.4625\n",
      "Epoch 71/150\n",
      "1558/1558 - 32s - loss: 2.5240 - accuracy: 0.4669\n",
      "Epoch 72/150\n",
      "1558/1558 - 32s - loss: 2.5143 - accuracy: 0.4664\n",
      "Epoch 73/150\n",
      "1558/1558 - 32s - loss: 2.5014 - accuracy: 0.4710\n",
      "Epoch 74/150\n",
      "1558/1558 - 32s - loss: 2.4917 - accuracy: 0.4701\n",
      "Epoch 75/150\n",
      "1558/1558 - 33s - loss: 2.4727 - accuracy: 0.4733\n",
      "Epoch 76/150\n",
      "1558/1558 - 33s - loss: 2.4569 - accuracy: 0.4773\n",
      "Epoch 77/150\n",
      "1558/1558 - 33s - loss: 2.4454 - accuracy: 0.4779\n",
      "Epoch 78/150\n",
      "1558/1558 - 33s - loss: 2.4270 - accuracy: 0.4805\n",
      "Epoch 79/150\n",
      "1558/1558 - 33s - loss: 2.4187 - accuracy: 0.4828\n",
      "Epoch 80/150\n",
      "1558/1558 - 34s - loss: 2.4098 - accuracy: 0.4843\n",
      "Epoch 81/150\n",
      "1558/1558 - 32s - loss: 2.3992 - accuracy: 0.4845\n",
      "Epoch 82/150\n",
      "1558/1558 - 32s - loss: 2.3774 - accuracy: 0.4907\n",
      "Epoch 83/150\n",
      "1558/1558 - 32s - loss: 2.3782 - accuracy: 0.4886\n",
      "Epoch 84/150\n",
      "1558/1558 - 32s - loss: 2.3576 - accuracy: 0.4929\n",
      "Epoch 85/150\n",
      "1558/1558 - 32s - loss: 2.3508 - accuracy: 0.4917\n",
      "Epoch 86/150\n",
      "1558/1558 - 32s - loss: 2.3370 - accuracy: 0.4946\n",
      "Epoch 87/150\n",
      "1558/1558 - 32s - loss: 2.3250 - accuracy: 0.4990\n",
      "Epoch 88/150\n",
      "1558/1558 - 32s - loss: 2.3207 - accuracy: 0.4980\n",
      "Epoch 89/150\n",
      "1558/1558 - 32s - loss: 2.3073 - accuracy: 0.5000\n",
      "Epoch 90/150\n",
      "1558/1558 - 34s - loss: 2.3009 - accuracy: 0.5011\n",
      "Epoch 91/150\n",
      "1558/1558 - 34s - loss: 2.2782 - accuracy: 0.5072\n",
      "Epoch 92/150\n",
      "1558/1558 - 34s - loss: 2.2777 - accuracy: 0.5067\n",
      "Epoch 93/150\n",
      "1558/1558 - 34s - loss: 2.2632 - accuracy: 0.5091\n",
      "Epoch 94/150\n",
      "1558/1558 - 34s - loss: 2.2515 - accuracy: 0.5095\n",
      "Epoch 95/150\n",
      "1558/1558 - 33s - loss: 2.2477 - accuracy: 0.5108\n",
      "Epoch 96/150\n",
      "1558/1558 - 32s - loss: 2.2357 - accuracy: 0.5118\n",
      "Epoch 97/150\n",
      "1558/1558 - 32s - loss: 2.2295 - accuracy: 0.5157\n",
      "Epoch 98/150\n",
      "1558/1558 - 32s - loss: 2.2212 - accuracy: 0.5165\n",
      "Epoch 99/150\n",
      "1558/1558 - 32s - loss: 2.2133 - accuracy: 0.5170\n",
      "Epoch 100/150\n",
      "1558/1558 - 32s - loss: 2.2008 - accuracy: 0.5184\n",
      "Epoch 101/150\n",
      "1558/1558 - 33s - loss: 2.1942 - accuracy: 0.5189\n",
      "Epoch 102/150\n",
      "1558/1558 - 32s - loss: 2.1851 - accuracy: 0.5223\n",
      "Epoch 103/150\n",
      "1558/1558 - 32s - loss: 2.1831 - accuracy: 0.5206\n",
      "Epoch 104/150\n",
      "1558/1558 - 33s - loss: 2.1652 - accuracy: 0.5275\n",
      "Epoch 105/150\n",
      "1558/1558 - 33s - loss: 2.1638 - accuracy: 0.5251\n",
      "Epoch 106/150\n",
      "1558/1558 - 35s - loss: 2.1506 - accuracy: 0.5298\n",
      "Epoch 107/150\n",
      "1558/1558 - 33s - loss: 2.1427 - accuracy: 0.5289\n",
      "Epoch 108/150\n",
      "1558/1558 - 33s - loss: 2.1399 - accuracy: 0.5292\n",
      "Epoch 109/150\n",
      "1558/1558 - 33s - loss: 2.1292 - accuracy: 0.5318\n",
      "Epoch 110/150\n",
      "1558/1558 - 33s - loss: 2.1360 - accuracy: 0.5296\n",
      "Epoch 111/150\n",
      "1558/1558 - 33s - loss: 2.1117 - accuracy: 0.5345\n",
      "Epoch 112/150\n",
      "1558/1558 - 33s - loss: 2.1037 - accuracy: 0.5341\n",
      "Epoch 113/150\n",
      "1558/1558 - 32s - loss: 2.1115 - accuracy: 0.5323\n",
      "Epoch 114/150\n",
      "1558/1558 - 33s - loss: 2.1005 - accuracy: 0.5357\n",
      "Epoch 115/150\n",
      "1558/1558 - 32s - loss: 2.0855 - accuracy: 0.5400\n",
      "Epoch 116/150\n",
      "1558/1558 - 33s - loss: 2.0898 - accuracy: 0.5368\n",
      "Epoch 117/150\n",
      "1558/1558 - 32s - loss: 2.0809 - accuracy: 0.5409\n",
      "Epoch 118/150\n",
      "1558/1558 - 33s - loss: 2.0612 - accuracy: 0.5432\n",
      "Epoch 119/150\n",
      "1558/1558 - 33s - loss: 2.0642 - accuracy: 0.5430\n",
      "Epoch 120/150\n",
      "1558/1558 - 33s - loss: 2.0506 - accuracy: 0.5453\n",
      "Epoch 121/150\n",
      "1558/1558 - 33s - loss: 2.0468 - accuracy: 0.5442\n",
      "Epoch 122/150\n",
      "1558/1558 - 33s - loss: 2.0388 - accuracy: 0.5471\n",
      "Epoch 123/150\n",
      "1558/1558 - 33s - loss: 2.0331 - accuracy: 0.5482\n",
      "Epoch 124/150\n",
      "1558/1558 - 34s - loss: 2.0359 - accuracy: 0.5482\n",
      "Epoch 125/150\n",
      "1558/1558 - 36s - loss: 2.0227 - accuracy: 0.5513\n",
      "Epoch 126/150\n",
      "1558/1558 - 33s - loss: 2.0183 - accuracy: 0.5512\n",
      "Epoch 127/150\n",
      "1558/1558 - 32s - loss: 2.0113 - accuracy: 0.5524\n",
      "Epoch 128/150\n",
      "1558/1558 - 33s - loss: 2.0069 - accuracy: 0.5528\n",
      "Epoch 129/150\n",
      "1558/1558 - 32s - loss: 1.9973 - accuracy: 0.5556\n",
      "Epoch 130/150\n",
      "1558/1558 - 33s - loss: 1.9831 - accuracy: 0.5564\n",
      "Epoch 131/150\n",
      "1558/1558 - 32s - loss: 1.9893 - accuracy: 0.5557\n",
      "Epoch 132/150\n",
      "1558/1558 - 32s - loss: 1.9936 - accuracy: 0.5535\n",
      "Epoch 133/150\n",
      "1558/1558 - 32s - loss: 1.9679 - accuracy: 0.5593\n",
      "Epoch 134/150\n",
      "1558/1558 - 33s - loss: 1.9734 - accuracy: 0.5589\n",
      "Epoch 135/150\n",
      "1558/1558 - 34s - loss: 1.9668 - accuracy: 0.5588\n",
      "Epoch 136/150\n",
      "1558/1558 - 34s - loss: 1.9563 - accuracy: 0.5619\n",
      "Epoch 137/150\n",
      "1558/1558 - 33s - loss: 1.9656 - accuracy: 0.5609\n",
      "Epoch 138/150\n",
      "1558/1558 - 33s - loss: 1.9453 - accuracy: 0.5646\n",
      "Epoch 139/150\n",
      "1558/1558 - 33s - loss: 1.9479 - accuracy: 0.5634\n",
      "Epoch 140/150\n",
      "1558/1558 - 32s - loss: 1.9307 - accuracy: 0.5672\n",
      "Epoch 141/150\n",
      "1558/1558 - 33s - loss: 1.9548 - accuracy: 0.5621\n",
      "Epoch 142/150\n",
      "1558/1558 - 33s - loss: 1.9360 - accuracy: 0.5638\n",
      "Epoch 143/150\n",
      "1558/1558 - 35s - loss: 1.9180 - accuracy: 0.5707\n",
      "Epoch 144/150\n",
      "1558/1558 - 33s - loss: 1.9246 - accuracy: 0.5667\n",
      "Epoch 145/150\n",
      "1558/1558 - 33s - loss: 1.9231 - accuracy: 0.5678\n",
      "Epoch 146/150\n",
      "1558/1558 - 33s - loss: 1.9141 - accuracy: 0.5694\n",
      "Epoch 147/150\n",
      "1558/1558 - 32s - loss: 1.9049 - accuracy: 0.5712\n",
      "Epoch 148/150\n",
      "1558/1558 - 33s - loss: 1.8987 - accuracy: 0.5729\n",
      "Epoch 149/150\n",
      "1558/1558 - 33s - loss: 1.9057 - accuracy: 0.5709\n",
      "Epoch 150/150\n",
      "1558/1558 - 33s - loss: 1.8875 - accuracy: 0.5748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3c38d8e550>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "# from soynlp.hangle import levenshtein\n",
    "# # from PreProcessing.find_common_part\n",
    "# from konlpy.tag import *\n",
    "# from PreProcessing import find_common_part\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 불러올때    \n",
    "with open('/content/drive/Shared drives/BigData/team__KU/data/result.txt', 'rb') as f:\n",
    "    result = pickle.load(f)\n",
    "\n",
    "result_gensim_input = [_.split() for _ in result if _ != '']\n",
    "result_tokenizer_input = [v for i, v in enumerate(result) if i%2 == 0 and v != '']\n",
    "ebs_in_result_for_getting_max_len = [v for i, v in enumerate(result_gensim_input) if i%2 == 0 and v != []]\n",
    "ebs_in_result_gensim_input = [v for i, v in enumerate(result_gensim_input) if i%2 == 0 and v != [] ]\n",
    "google_in_result_gensim_input = [v for i, v in enumerate(result_gensim_input) if i%2 == 1 and v != []]\n",
    "    \n",
    "# Generate EBS string vectors matrix\n",
    "ws = 1\n",
    "es = 30\n",
    "\n",
    "model_cbow = Word2Vec(\n",
    "                ebs_in_result_gensim_input, \n",
    "                window = ws,\n",
    "                size =es,\n",
    "                min_count=1,\n",
    "                workers = 10\n",
    "                )\n",
    "vocabs = list(model_cbow.wv.index2word)\n",
    "embedding_matrix = np.zeros((len(vocabs), es))\n",
    "for i, w in enumerate(vocabs):\n",
    "    embedding_matrix[i] = model_cbow[w]\n",
    "\n",
    "#embedding_matrix 맨 위에 0벡터 추가\n",
    "stacked_zero = np.zeros((1, es))\n",
    "embedding_matrix = np.vstack((stacked_zero, embedding_matrix))\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(result_tokenizer_input)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "#시퀀스 만들기\n",
    "sequences = list()\n",
    "\n",
    "for line in result_tokenizer_input: # 1,214 개의 샘플에 대해서 샘플을 1개씩 가져온다.\n",
    "    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[i-9 if i > 9 else 0:i+1]\n",
    "        sequences.append(sequence)\n",
    "        \n",
    "max_len=max(len(l) for l in sequences)\n",
    "print('max length : {}'.format(max_len))\n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            es,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "# y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1\n",
    "model.add(LSTM(128, activation = 'relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=150, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gr92uvp-em6R"
   },
   "source": [
    "# --------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "colab learning_index_rev-150.ipynb의 사본",
   "provenance": [
    {
     "file_id": "1Kv1Q8tBSEohDG79-pdfqLF837km7FSXu",
     "timestamp": 1599317944329
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
