{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"colab learning_pre_e200_vb2.ipynb의 사본","provenance":[{"file_id":"1194sb6oCq5sGHpIgmoZNfntKJzW_4g3o","timestamp":1599551454075}]}},"cells":[{"cell_type":"code","metadata":{"id":"31nAS8hSMH6s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1599542991788,"user_tz":-540,"elapsed":29056,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}},"outputId":"34f4a463-d6ee-47d5-9266-4e94edbcf58a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V1OK2pP0MH6z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599550523436,"user_tz":-540,"elapsed":7514797,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}},"outputId":"efe1f523-d41b-4dd0-9ec0-2a92836d35c0"},"source":["import pickle\n","# from soynlp.hangle import levenshtein\n","# # from PreProcessing.find_common_part\n","# from konlpy.tag import *\n","# from PreProcessing import find_common_part\n","import numpy as np\n","from gensim.models import Word2Vec\n","from keras.layers import Embedding\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Dense, LSTM\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","# 불러올때    \n","with open('/content/drive/Shared drives/BigData/team__KU/data/result.txt', 'rb') as f:\n","    result = pickle.load(f)\n","\n","# # 불러올때    \n","# with open('data/result.txt', 'rb') as f:\n","#     result = pickle.load(f)\n","\n","result_gensim_input = [_.split() for _ in result if _ != '']\n","result_tokenizer_input = [v for i, v in enumerate(result) if i%2 == 0 and v != '']\n","ebs_in_result_for_getting_max_len = [v for i, v in enumerate(result_gensim_input) if i%2 == 0 and v != []]\n","ebs_in_result_gensim_input = [v for i, v in enumerate(result_gensim_input) if i%2 == 0 and v != [] ]\n","google_in_result_gensim_input = [v for i, v in enumerate(result_gensim_input) if i%2 == 1 and v != []]\n","    \n","# Generate EBS string vectors matrix\n","ws = 1\n","es = 30\n","\n","model_cbow = Word2Vec(\n","                ebs_in_result_gensim_input, \n","                window = ws,\n","                size =es,\n","                min_count=1,\n","                workers = 10\n","                )\n","\n","t = Tokenizer()\n","t.fit_on_texts(result_tokenizer_input)\n","vocab_size = len(t.word_index) + 1\n","print('단어 집합의 크기 : %d' % vocab_size)\n","\n","embedding_matrix = np.zeros((vocab_size, es))\n","for w, i in t.word_index.items():\n","    embedding_matrix[i] = model_cbow[w]\n","print('embedding_matrix.shape :{}'.format(embedding_matrix.shape))\n","\n","#시퀀스 만들기\n","sequences = list()\n","\n","for line in result_tokenizer_input: # 1,214 개의 샘플에 대해서 샘플을 1개씩 가져온다.\n","    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩\n","    for i in range(1, len(encoded)):\n","        sequence = encoded[i-9 if i > 9 else 0:i+1]\n","        sequences.append(sequence)\n","\n","max_len=max(len(l) for l in sequences)\n","print('샘플의 최대 길이 : {}'.format(max_len))\n","\n","sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n","\n","sequences = np.array(sequences)\n","X = sequences[:,:-1]\n","y = sequences[:,-1]\n","\n","y = to_categorical(y, num_classes=vocab_size)\n","\n","embedding_layer = Embedding(vocab_size,\n","                            es,\n","                            weights=[embedding_matrix],\n","                            input_length=max_len-1,\n","                            trainable=False)\n","\n","model = Sequential()\n","model.add(embedding_layer)\n","# y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1\n","model.add(LSTM(128, activation='relu'))\n","model.add(Dense(vocab_size, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X, y, batch_size = 40, epochs=200, verbose=2)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["단어 집합의 크기 : 12168\n","embedding_matrix.shape :(12168, 30)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"},{"output_type":"stream","text":["샘플의 최대 길이 : 10\n","Epoch 1/200\n","1247/1247 - 37s - loss: 7.7572 - accuracy: 0.0169\n","Epoch 2/200\n","1247/1247 - 37s - loss: 7.4231 - accuracy: 0.0200\n","Epoch 3/200\n","1247/1247 - 38s - loss: 7.2639 - accuracy: 0.0241\n","Epoch 4/200\n","1247/1247 - 37s - loss: 7.1362 - accuracy: 0.0247\n","Epoch 5/200\n","1247/1247 - 37s - loss: 7.0271 - accuracy: 0.0254\n","Epoch 6/200\n","1247/1247 - 38s - loss: 6.9205 - accuracy: 0.0259\n","Epoch 7/200\n","1247/1247 - 37s - loss: 6.8118 - accuracy: 0.0275\n","Epoch 8/200\n","1247/1247 - 37s - loss: 6.6894 - accuracy: 0.0292\n","Epoch 9/200\n","1247/1247 - 37s - loss: 6.5458 - accuracy: 0.0320\n","Epoch 10/200\n","1247/1247 - 37s - loss: 6.3993 - accuracy: 0.0327\n","Epoch 11/200\n","1247/1247 - 37s - loss: 6.2520 - accuracy: 0.0347\n","Epoch 12/200\n","1247/1247 - 37s - loss: 6.1027 - accuracy: 0.0359\n","Epoch 13/200\n","1247/1247 - 37s - loss: 5.9405 - accuracy: 0.0359\n","Epoch 14/200\n","1247/1247 - 37s - loss: 5.7608 - accuracy: 0.0382\n","Epoch 15/200\n","1247/1247 - 38s - loss: 5.5700 - accuracy: 0.0457\n","Epoch 16/200\n","1247/1247 - 37s - loss: 5.3793 - accuracy: 0.0638\n","Epoch 17/200\n","1247/1247 - 37s - loss: 5.2096 - accuracy: 0.0887\n","Epoch 18/200\n","1247/1247 - 37s - loss: 5.0633 - accuracy: 0.1131\n","Epoch 19/200\n","1247/1247 - 37s - loss: 4.9308 - accuracy: 0.1355\n","Epoch 20/200\n","1247/1247 - 38s - loss: 4.8140 - accuracy: 0.1522\n","Epoch 21/200\n","1247/1247 - 37s - loss: 4.7065 - accuracy: 0.1677\n","Epoch 22/200\n","1247/1247 - 37s - loss: 4.6067 - accuracy: 0.1823\n","Epoch 23/200\n","1247/1247 - 39s - loss: 4.5150 - accuracy: 0.1929\n","Epoch 24/200\n","1247/1247 - 37s - loss: 4.4328 - accuracy: 0.2040\n","Epoch 25/200\n","1247/1247 - 37s - loss: 4.3504 - accuracy: 0.2142\n","Epoch 26/200\n","1247/1247 - 37s - loss: 4.2769 - accuracy: 0.2235\n","Epoch 27/200\n","1247/1247 - 37s - loss: 4.2001 - accuracy: 0.2307\n","Epoch 28/200\n","1247/1247 - 38s - loss: 4.1268 - accuracy: 0.2413\n","Epoch 29/200\n","1247/1247 - 37s - loss: 4.0588 - accuracy: 0.2481\n","Epoch 30/200\n","1247/1247 - 37s - loss: 3.9949 - accuracy: 0.2551\n","Epoch 31/200\n","1247/1247 - 37s - loss: 3.9344 - accuracy: 0.2631\n","Epoch 32/200\n","1247/1247 - 38s - loss: 3.8725 - accuracy: 0.2688\n","Epoch 33/200\n","1247/1247 - 37s - loss: 3.8155 - accuracy: 0.2777\n","Epoch 34/200\n","1247/1247 - 37s - loss: 3.7615 - accuracy: 0.2838\n","Epoch 35/200\n","1247/1247 - 37s - loss: 3.7127 - accuracy: 0.2896\n","Epoch 36/200\n","1247/1247 - 38s - loss: 3.6632 - accuracy: 0.2966\n","Epoch 37/200\n","1247/1247 - 37s - loss: 3.6163 - accuracy: 0.3031\n","Epoch 38/200\n","1247/1247 - 37s - loss: 3.5743 - accuracy: 0.3066\n","Epoch 39/200\n","1247/1247 - 37s - loss: 3.5282 - accuracy: 0.3131\n","Epoch 40/200\n","1247/1247 - 38s - loss: 3.4889 - accuracy: 0.3173\n","Epoch 41/200\n","1247/1247 - 37s - loss: 3.4496 - accuracy: 0.3228\n","Epoch 42/200\n","1247/1247 - 37s - loss: 3.4127 - accuracy: 0.3291\n","Epoch 43/200\n","1247/1247 - 37s - loss: 3.3754 - accuracy: 0.3331\n","Epoch 44/200\n","1247/1247 - 37s - loss: 3.3438 - accuracy: 0.3386\n","Epoch 45/200\n","1247/1247 - 37s - loss: 3.3087 - accuracy: 0.3428\n","Epoch 46/200\n","1247/1247 - 37s - loss: 3.2790 - accuracy: 0.3480\n","Epoch 47/200\n","1247/1247 - 37s - loss: 3.2443 - accuracy: 0.3519\n","Epoch 48/200\n","1247/1247 - 38s - loss: 3.2178 - accuracy: 0.3547\n","Epoch 49/200\n","1247/1247 - 38s - loss: 3.1872 - accuracy: 0.3593\n","Epoch 50/200\n","1247/1247 - 37s - loss: 3.1576 - accuracy: 0.3648\n","Epoch 51/200\n","1247/1247 - 37s - loss: 3.1339 - accuracy: 0.3661\n","Epoch 52/200\n","1247/1247 - 37s - loss: 3.1076 - accuracy: 0.3711\n","Epoch 53/200\n","1247/1247 - 37s - loss: 3.0759 - accuracy: 0.3749\n","Epoch 54/200\n","1247/1247 - 37s - loss: 3.0536 - accuracy: 0.3799\n","Epoch 55/200\n","1247/1247 - 37s - loss: 3.0322 - accuracy: 0.3815\n","Epoch 56/200\n","1247/1247 - 37s - loss: 3.0114 - accuracy: 0.3843\n","Epoch 57/200\n","1247/1247 - 38s - loss: 2.9870 - accuracy: 0.3874\n","Epoch 58/200\n","1247/1247 - 37s - loss: 2.9661 - accuracy: 0.3911\n","Epoch 59/200\n","1247/1247 - 37s - loss: 2.9418 - accuracy: 0.3955\n","Epoch 60/200\n","1247/1247 - 37s - loss: 2.9250 - accuracy: 0.3965\n","Epoch 61/200\n","1247/1247 - 38s - loss: 2.9075 - accuracy: 0.3996\n","Epoch 62/200\n","1247/1247 - 37s - loss: 2.8810 - accuracy: 0.4057\n","Epoch 63/200\n","1247/1247 - 37s - loss: 2.8628 - accuracy: 0.4064\n","Epoch 64/200\n","1247/1247 - 37s - loss: 2.8451 - accuracy: 0.4121\n","Epoch 65/200\n","1247/1247 - 38s - loss: 2.8311 - accuracy: 0.4122\n","Epoch 66/200\n","1247/1247 - 39s - loss: 2.8097 - accuracy: 0.4150\n","Epoch 67/200\n","1247/1247 - 38s - loss: 2.7940 - accuracy: 0.4187\n","Epoch 68/200\n","1247/1247 - 37s - loss: 2.7744 - accuracy: 0.4200\n","Epoch 69/200\n","1247/1247 - 38s - loss: 2.7623 - accuracy: 0.4230\n","Epoch 70/200\n","1247/1247 - 37s - loss: 2.7452 - accuracy: 0.4256\n","Epoch 71/200\n","1247/1247 - 37s - loss: 2.7288 - accuracy: 0.4281\n","Epoch 72/200\n","1247/1247 - 37s - loss: 2.7113 - accuracy: 0.4305\n","Epoch 73/200\n","1247/1247 - 37s - loss: 2.6958 - accuracy: 0.4337\n","Epoch 74/200\n","1247/1247 - 38s - loss: 2.6797 - accuracy: 0.4347\n","Epoch 75/200\n","1247/1247 - 37s - loss: 2.6670 - accuracy: 0.4381\n","Epoch 76/200\n","1247/1247 - 37s - loss: 2.6554 - accuracy: 0.4392\n","Epoch 77/200\n","1247/1247 - 38s - loss: 2.6415 - accuracy: 0.4412\n","Epoch 78/200\n","1247/1247 - 37s - loss: 2.6274 - accuracy: 0.4432\n","Epoch 79/200\n","1247/1247 - 37s - loss: 2.6148 - accuracy: 0.4458\n","Epoch 80/200\n","1247/1247 - 37s - loss: 2.5975 - accuracy: 0.4488\n","Epoch 81/200\n","1247/1247 - 37s - loss: 2.5899 - accuracy: 0.4498\n","Epoch 82/200\n","1247/1247 - 37s - loss: 2.5790 - accuracy: 0.4507\n","Epoch 83/200\n","1247/1247 - 38s - loss: 2.5600 - accuracy: 0.4558\n","Epoch 84/200\n","1247/1247 - 37s - loss: 2.5537 - accuracy: 0.4547\n","Epoch 85/200\n","1247/1247 - 37s - loss: 2.5376 - accuracy: 0.4583\n","Epoch 86/200\n","1247/1247 - 38s - loss: 2.5322 - accuracy: 0.4586\n","Epoch 87/200\n","1247/1247 - 37s - loss: 2.5164 - accuracy: 0.4608\n","Epoch 88/200\n","1247/1247 - 37s - loss: 2.5033 - accuracy: 0.4637\n","Epoch 89/200\n","1247/1247 - 37s - loss: 2.4948 - accuracy: 0.4666\n","Epoch 90/200\n","1247/1247 - 37s - loss: 2.4798 - accuracy: 0.4692\n","Epoch 91/200\n","1247/1247 - 38s - loss: 2.4713 - accuracy: 0.4685\n","Epoch 92/200\n","1247/1247 - 37s - loss: 2.4553 - accuracy: 0.4719\n","Epoch 93/200\n","1247/1247 - 37s - loss: 2.4536 - accuracy: 0.4721\n","Epoch 94/200\n","1247/1247 - 38s - loss: 2.4363 - accuracy: 0.4746\n","Epoch 95/200\n","1247/1247 - 37s - loss: 2.4283 - accuracy: 0.4771\n","Epoch 96/200\n","1247/1247 - 37s - loss: 2.4277 - accuracy: 0.4745\n","Epoch 97/200\n","1247/1247 - 37s - loss: 2.4125 - accuracy: 0.4793\n","Epoch 98/200\n","1247/1247 - 37s - loss: 2.3943 - accuracy: 0.4816\n","Epoch 99/200\n","1247/1247 - 37s - loss: 2.3976 - accuracy: 0.4808\n","Epoch 100/200\n","1247/1247 - 38s - loss: 2.3773 - accuracy: 0.4842\n","Epoch 101/200\n","1247/1247 - 37s - loss: 2.3745 - accuracy: 0.4853\n","Epoch 102/200\n","1247/1247 - 38s - loss: 2.3589 - accuracy: 0.4881\n","Epoch 103/200\n","1247/1247 - 37s - loss: 2.3568 - accuracy: 0.4879\n","Epoch 104/200\n","1247/1247 - 37s - loss: 2.3438 - accuracy: 0.4887\n","Epoch 105/200\n","1247/1247 - 37s - loss: 2.3323 - accuracy: 0.4923\n","Epoch 106/200\n","1247/1247 - 37s - loss: 2.3344 - accuracy: 0.4918\n","Epoch 107/200\n","1247/1247 - 37s - loss: 2.3169 - accuracy: 0.4945\n","Epoch 108/200\n","1247/1247 - 38s - loss: 2.3117 - accuracy: 0.4948\n","Epoch 109/200\n","1247/1247 - 37s - loss: 2.3024 - accuracy: 0.4970\n","Epoch 110/200\n","1247/1247 - 38s - loss: 2.2940 - accuracy: 0.4997\n","Epoch 111/200\n","1247/1247 - 37s - loss: 2.2868 - accuracy: 0.5004\n","Epoch 112/200\n","1247/1247 - 37s - loss: 2.2795 - accuracy: 0.5012\n","Epoch 113/200\n","1247/1247 - 37s - loss: 2.2790 - accuracy: 0.5023\n","Epoch 114/200\n","1247/1247 - 37s - loss: 2.2602 - accuracy: 0.5039\n","Epoch 115/200\n","1247/1247 - 37s - loss: 2.2486 - accuracy: 0.5058\n","Epoch 116/200\n","1247/1247 - 38s - loss: 2.2492 - accuracy: 0.5059\n","Epoch 117/200\n","1247/1247 - 39s - loss: 2.2500 - accuracy: 0.5046\n","Epoch 118/200\n","1247/1247 - 38s - loss: 2.2290 - accuracy: 0.5119\n","Epoch 119/200\n","1247/1247 - 38s - loss: 2.2311 - accuracy: 0.5090\n","Epoch 120/200\n","1247/1247 - 37s - loss: 2.2198 - accuracy: 0.5117\n","Epoch 121/200\n","1247/1247 - 37s - loss: 2.2018 - accuracy: 0.5144\n","Epoch 122/200\n","1247/1247 - 37s - loss: 2.2228 - accuracy: 0.5104\n","Epoch 123/200\n","1247/1247 - 37s - loss: 2.1918 - accuracy: 0.5167\n","Epoch 124/200\n","1247/1247 - 37s - loss: 2.1932 - accuracy: 0.5135\n","Epoch 125/200\n","1247/1247 - 38s - loss: 2.1904 - accuracy: 0.5164\n","Epoch 126/200\n","1247/1247 - 37s - loss: 2.1753 - accuracy: 0.5200\n","Epoch 127/200\n","1247/1247 - 38s - loss: 2.1762 - accuracy: 0.5189\n","Epoch 128/200\n","1247/1247 - 37s - loss: 2.1656 - accuracy: 0.5184\n","Epoch 129/200\n","1247/1247 - 37s - loss: 2.1723 - accuracy: 0.5187\n","Epoch 130/200\n","1247/1247 - 37s - loss: 2.1540 - accuracy: 0.5222\n","Epoch 131/200\n","1247/1247 - 37s - loss: 2.1374 - accuracy: 0.5255\n","Epoch 132/200\n","1247/1247 - 37s - loss: 2.1534 - accuracy: 0.5225\n","Epoch 133/200\n","1247/1247 - 38s - loss: 2.1351 - accuracy: 0.5256\n","Epoch 134/200\n","1247/1247 - 38s - loss: 2.1218 - accuracy: 0.5288\n","Epoch 135/200\n","1247/1247 - 38s - loss: 2.1295 - accuracy: 0.5271\n","Epoch 136/200\n","1247/1247 - 37s - loss: 2.1174 - accuracy: 0.5307\n","Epoch 137/200\n","1247/1247 - 37s - loss: 2.1115 - accuracy: 0.5297\n","Epoch 138/200\n","1247/1247 - 37s - loss: 2.1122 - accuracy: 0.5305\n","Epoch 139/200\n","1247/1247 - 37s - loss: 2.1107 - accuracy: 0.5287\n","Epoch 140/200\n","1247/1247 - 37s - loss: 2.0981 - accuracy: 0.5317\n","Epoch 141/200\n","1247/1247 - 37s - loss: 2.0790 - accuracy: 0.5350\n","Epoch 142/200\n","1247/1247 - 38s - loss: 2.0844 - accuracy: 0.5341\n","Epoch 143/200\n","1247/1247 - 38s - loss: 2.0805 - accuracy: 0.5356\n","Epoch 144/200\n","1247/1247 - 37s - loss: 2.0824 - accuracy: 0.5349\n","Epoch 145/200\n","1247/1247 - 37s - loss: 2.0665 - accuracy: 0.5377\n","Epoch 146/200\n","1247/1247 - 37s - loss: 2.0617 - accuracy: 0.5387\n","Epoch 147/200\n","1247/1247 - 37s - loss: 2.0708 - accuracy: 0.5386\n","Epoch 148/200\n","1247/1247 - 37s - loss: 2.0567 - accuracy: 0.5398\n","Epoch 149/200\n","1247/1247 - 37s - loss: 2.0549 - accuracy: 0.5398\n","Epoch 150/200\n","1247/1247 - 38s - loss: 2.0409 - accuracy: 0.5427\n","Epoch 151/200\n","1247/1247 - 38s - loss: 2.0421 - accuracy: 0.5417\n","Epoch 152/200\n","1247/1247 - 37s - loss: 2.0234 - accuracy: 0.5465\n","Epoch 153/200\n","1247/1247 - 37s - loss: 2.0398 - accuracy: 0.5409\n","Epoch 154/200\n","1247/1247 - 37s - loss: 2.0310 - accuracy: 0.5441\n","Epoch 155/200\n","1247/1247 - 37s - loss: 2.0034 - accuracy: 0.5512\n","Epoch 156/200\n","1247/1247 - 37s - loss: 2.0152 - accuracy: 0.5471\n","Epoch 157/200\n","1247/1247 - 37s - loss: 2.0042 - accuracy: 0.5490\n","Epoch 158/200\n","1247/1247 - 37s - loss: 2.0081 - accuracy: 0.5495\n","Epoch 159/200\n","1247/1247 - 38s - loss: 1.9956 - accuracy: 0.5517\n","Epoch 160/200\n","1247/1247 - 37s - loss: 1.9983 - accuracy: 0.5494\n","Epoch 161/200\n","1247/1247 - 37s - loss: 1.9992 - accuracy: 0.5512\n","Epoch 162/200\n","1247/1247 - 37s - loss: 1.9833 - accuracy: 0.5522\n","Epoch 163/200\n","1247/1247 - 37s - loss: 1.9817 - accuracy: 0.5525\n","Epoch 164/200\n","1247/1247 - 37s - loss: 1.9811 - accuracy: 0.5526\n","Epoch 165/200\n","1247/1247 - 37s - loss: 1.9743 - accuracy: 0.5535\n","Epoch 166/200\n","1247/1247 - 38s - loss: 1.9873 - accuracy: 0.5524\n","Epoch 167/200\n","1247/1247 - 39s - loss: 1.9589 - accuracy: 0.5592\n","Epoch 168/200\n","1247/1247 - 40s - loss: 1.9644 - accuracy: 0.5560\n","Epoch 169/200\n","1247/1247 - 37s - loss: 1.9766 - accuracy: 0.5543\n","Epoch 170/200\n","1247/1247 - 37s - loss: 1.9484 - accuracy: 0.5595\n","Epoch 171/200\n","1247/1247 - 37s - loss: 1.9486 - accuracy: 0.5605\n","Epoch 172/200\n","1247/1247 - 37s - loss: 1.9556 - accuracy: 0.5597\n","Epoch 173/200\n","1247/1247 - 37s - loss: 1.9297 - accuracy: 0.5637\n","Epoch 174/200\n","1247/1247 - 37s - loss: 1.9369 - accuracy: 0.5611\n","Epoch 175/200\n","1247/1247 - 37s - loss: 1.9509 - accuracy: 0.5585\n","Epoch 176/200\n","1247/1247 - 39s - loss: 1.9246 - accuracy: 0.5655\n","Epoch 177/200\n","1247/1247 - 37s - loss: 1.9260 - accuracy: 0.5641\n","Epoch 178/200\n","1247/1247 - 37s - loss: 1.9182 - accuracy: 0.5644\n","Epoch 179/200\n","1247/1247 - 37s - loss: 1.9326 - accuracy: 0.5612\n","Epoch 180/200\n","1247/1247 - 37s - loss: 1.8970 - accuracy: 0.5711\n","Epoch 181/200\n","1247/1247 - 37s - loss: 1.9128 - accuracy: 0.5644\n","Epoch 182/200\n","1247/1247 - 37s - loss: 1.9212 - accuracy: 0.5648\n","Epoch 183/200\n","1247/1247 - 37s - loss: 1.9037 - accuracy: 0.5673\n","Epoch 184/200\n","1247/1247 - 41s - loss: 1.8948 - accuracy: 0.5709\n","Epoch 185/200\n","1247/1247 - 38s - loss: 1.8958 - accuracy: 0.5710\n","Epoch 186/200\n","1247/1247 - 37s - loss: 1.8888 - accuracy: 0.5712\n","Epoch 187/200\n","1247/1247 - 37s - loss: 1.8989 - accuracy: 0.5684\n","Epoch 188/200\n","1247/1247 - 37s - loss: 1.8968 - accuracy: 0.5683\n","Epoch 189/200\n","1247/1247 - 37s - loss: 1.8896 - accuracy: 0.5696\n","Epoch 190/200\n","1247/1247 - 37s - loss: 1.8882 - accuracy: 0.5703\n","Epoch 191/200\n","1247/1247 - 37s - loss: 1.8617 - accuracy: 0.5763\n","Epoch 192/200\n","1247/1247 - 38s - loss: 1.9021 - accuracy: 0.5688\n","Epoch 193/200\n","1247/1247 - 38s - loss: 1.8613 - accuracy: 0.5776\n","Epoch 194/200\n","1247/1247 - 38s - loss: 1.8625 - accuracy: 0.5759\n","Epoch 195/200\n","1247/1247 - 37s - loss: 1.8678 - accuracy: 0.5716\n","Epoch 196/200\n","1247/1247 - 37s - loss: 1.8463 - accuracy: 0.5801\n","Epoch 197/200\n","1247/1247 - 38s - loss: 1.8564 - accuracy: 0.5762\n","Epoch 198/200\n","1247/1247 - 37s - loss: 1.8550 - accuracy: 0.5763\n","Epoch 199/200\n","1247/1247 - 37s - loss: 1.8578 - accuracy: 0.5766\n","Epoch 200/200\n","1247/1247 - 38s - loss: 1.8511 - accuracy: 0.5778\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f06fb990550>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"O5M0Q_YzxEgG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599550524473,"user_tz":-540,"elapsed":1025,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}}},"source":["def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n","    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n","    sentence = ''\n","    for _ in range(n): # n번 반복\n","        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n","        encoded = pad_sequences([encoded], maxlen=23, padding='pre') # 데이터에 대한 패딩\n","        result = model.predict_classes(encoded, verbose=0)\n","    # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n","        for word, index in t.word_index.items(): \n","            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n","                break # 해당 단어가 예측 단어이므로 break\n","        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n","        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n","    # for문이므로 이 행동을 다시 반복\n","    sentence = init_word + sentence\n","    return sentence"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-y7NFF-q1OS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599551014463,"user_tz":-540,"elapsed":784,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GF-GK6ktq9t7","colab_type":"text"},"source":["# pre, e = 200, verbose = 2, 인덱스 맞춘것 확인한 후, trainable = False"]},{"cell_type":"code","metadata":{"id":"EDdFGLqGN9tU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1599550524755,"user_tz":-540,"elapsed":1292,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}},"outputId":"421852c5-1522-475e-968b-d53a59d940db"},"source":["print(sentence_generation(model, t, '다시', 10))\n","# 다시 한번 첫 시작 지점 뭐라고 이렇게 둘 더한것이십육이 : EBS\n","# 다시 한번 첫 시작 기점 뭐라고 이렇게 둘 도아것이십육일이 : GOOGLE"],"execution_count":4,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-3-4fcb1994ebdf>:7: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n","Instructions for updating:\n","Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","WARNING:tensorflow:Model was constructed with shape (None, 9) for input Tensor(\"embedding_input:0\", shape=(None, 9), dtype=float32), but it was called on an input with incompatible shape (None, 23).\n","다시 읽는 읽는 사인에이 알기위해 부탁드리겠습니다 계속 뭐됩니까밑변높이 넘어가나요 이조 하나로\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SI5wKddTOBD3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599550525401,"user_tz":-540,"elapsed":1923,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}},"outputId":"e4c03483-cfda-4a9b-a7a0-2de23a4c5d6b"},"source":["print(sentence_generation(model, t, '주황색', 10))\n","# 주황색 변해요 길어지죠 빗변 길이 어떻게 되죠 길어집니다\n","# 주황색 변해요 빗변 길이 너어떻게 되죠 길어집니다"],"execution_count":5,"outputs":[{"output_type":"stream","text":["주황색 읽는 읽는 이치로길이길이표현하는게어렵겠죠삼분 이치로길이길이표현하는게어렵겠죠삼분 읽는 이개 성질 풀거예요심지어 처음파란색 봐주시면\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oeW3_-AUOqZq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599550525712,"user_tz":-540,"elapsed":2222,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}},"outputId":"384e6f16-5928-4487-eb63-1f7c71042c4e"},"source":["print(sentence_generation(model, t, '약속', 10))\n","# 약속 여러분 첫 삼각비 는걸 공부 하기에앞서서 약속 여러분 익히셔야 됩니다\n","# 약속 여러분 첫 삼각비 란걸 공부 기안써서이 약속 여러분 이틀쉬어야 됩니다"],"execution_count":6,"outputs":[{"output_type":"stream","text":["약속 읽는 읽는 옮겨와 옮겨와 읽는 텐데 사러 옮겨와 받아들일수있을테니 해봤습니다\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_hvS0FNoOqfU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599550526373,"user_tz":-540,"elapsed":2869,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}},"outputId":"493f6e66-4780-4272-e13b-31c00d929fed"},"source":["print(sentence_generation(model, t, '직각삼각형', 10))\n","# 직각삼각형 만약 우리 이각 기준 각놔 보겠습니다\n","# 직각삼각형 만약 우리 이각 기준 가기나 보겠습니다"],"execution_count":7,"outputs":[{"output_type":"stream","text":["직각삼각형 읽는 읽는 사인에이 라면 보셔야겠고요다음 집도 물어본다면 했으니까 있는내용삼각비 이백이십일\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vkNToN9rOqkY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599550526844,"user_tz":-540,"elapsed":3327,"user":{"displayName":"이경찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj56GLoLaRW56a8OPmpFhv1BaU-InaJEauJiWWicQ=s64","userId":"06819814280514699015"}},"outputId":"607ee5b7-9967-4da1-a5fb-e71324da154f"},"source":["print(sentence_generation(model, t, '두번째', 10))\n","# 두번째 탄젠트 에이 했습니다\n","# 두번째 탄젠트 얘기 했습니다"],"execution_count":8,"outputs":[{"output_type":"stream","text":["두번째 읽는 읽는 사인에이 알기위해 부탁드리겠습니다 보셔야겠고요다음 옮겨와 뭐였어밑변 풀됩니까에이치묶어내면 직선\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0q-LGVlso4uK","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}