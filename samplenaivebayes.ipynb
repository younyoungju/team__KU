{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "# Base class inspired by https://github.com/scikit-learn/scikit-learn/blob/a1860144aa2083277ba354b0cc46f9eb4acf0db0/sklearn/naive_bayes.py\n",
    "class NaiveBayes:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the Naive Bayes model to the input\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        y --  M x 1 numpy array, storing K unique labels\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _predict_log_proba(self, X):\n",
    "        \"\"\"Predict the log of the label probabilities for the input\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        Returns:\n",
    "        log_probabilities -- M x K numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        log_prob = logsumexp(jll, axis=1)\n",
    "        return jll - np.atleast_2d(log_prob).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict the label probabilities for the input\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        Returns:\n",
    "        probabilities -- M x K numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        return np.exp(self._predict_log_proba(X))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the labels for the input\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        Returns:\n",
    "        probabilities -- M x K numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        return self._classes[np.argmax(self._joint_log_likelihood(X), axis=1)]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"Accuracy for test data and expected labels\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        y --  M x 1 numpy array, storing K unique labels\n",
    "        Returns:\n",
    "        accuracy_score -- decimal value (0.0-1.0)\n",
    "        \"\"\"\n",
    "\n",
    "        pred = self.predict(X)\n",
    "\n",
    "        score = 0.0\n",
    "        for i in range(pred.shape[0]):\n",
    "            if (pred[i] == y[i]):\n",
    "                score += 1\n",
    "\n",
    "        return score / pred.shape[0]\n",
    "\n",
    "class GaussianBayes(NaiveBayes):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the Naive Bayes model to the input\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        y --  M x 1 numpy array, storing K unique labels\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        unq, unq_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        self._classes = unq # K x 1\n",
    "        self.priors = unq_counts / y.shape[0] # K x 1\n",
    "        self.num_classes = len(unq)\n",
    "\n",
    "        mean = []\n",
    "        var = []\n",
    "\n",
    "        for y_i in unq:\n",
    "            X_i = X[y == y_i, :]\n",
    "\n",
    "            mean.append(np.mean(X_i, axis=0))\n",
    "            var.append(np.var(X_i, axis=0))\n",
    "\n",
    "        self.mean = self._weights = np.vstack(mean) # K x N\n",
    "        self.var = np.vstack(var)  # K x N\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        prob = []\n",
    "\n",
    "        epsilon = 1e-9\n",
    "\n",
    "        for k in range(self.num_classes):\n",
    "            mean = self.mean[k, :]\n",
    "            var = self.var[k, :] + epsilon # add epsilon so we never divide by zero\n",
    "            gauss = -0.5 * np.sum(np.log(2.0 * np.pi * var))\n",
    "            gauss -= 0.5 * np.sum(np.square(X - mean) / var, axis=1)\n",
    "            prob.append(np.log(self.priors[k]) + gauss)\n",
    "\n",
    "        prob = np.vstack(prob).T\n",
    "        return prob\n",
    "\n",
    "class MultinomialBayes(NaiveBayes):\n",
    "\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the Naive Bayes model to the input\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        y --  M x 1 numpy array, storing K unique labels\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        unq, unq_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        self._classes = unq # K x 1\n",
    "        self._log_priors = np.log(unq_counts) - np.log(y.shape[0])\n",
    "\n",
    "        # Alpha will be used for smoothing later.\n",
    "        # If set to zero, we could have numerical instability.\n",
    "        if self.alpha == 0.0:\n",
    "            self.alpha = 1e-16\n",
    "\n",
    "        feature_log_probs = []\n",
    "        for k in range(len(unq)):\n",
    "            # Grab all data for the kth label\n",
    "            subset = X[y == k, :]\n",
    "\n",
    "            # We add alpha for smoothing. This means we don't take the\n",
    "            # log of zero in case a feature is missing (=> P(feature) = 0)\n",
    "            counts = np.sum(subset, axis=0) + self.alpha\n",
    "            count_sum = np.sum(counts) + self.alpha * 2\n",
    "\n",
    "            # Subtract the logs (same as division)\n",
    "            feature_log_probs.append(np.log(counts) - np.log(count_sum.reshape(-1,1)))\n",
    "\n",
    "        self._feature_log_prob = np.vstack(feature_log_probs)\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Predict the log of the label probabilities for the input\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        Returns:\n",
    "        log_probabilities -- M x K numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        # Multinomial Bayes is a simple linear classifier in log-space!\n",
    "        return self._log_priors + X.dot(self._feature_log_prob.T)\n",
    "\n",
    "class BernoulliBayes(NaiveBayes):\n",
    "\n",
    "    def __init__(self, alpha=1.0, binarize=0.5):\n",
    "        self.alpha = alpha\n",
    "        self.binarize = binarize\n",
    "\n",
    "    def __binarize(self, X):\n",
    "        X_bin = np.zeros(X.shape)\n",
    "        X_bin[X > self.binarize] = 1\n",
    "        return X_bin\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the Naive Bayes model to the input\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        y --  M x 1 numpy array, storing K unique labels\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        unq, unq_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        self._classes = unq # K x 1\n",
    "        self._priors = unq_counts / y.shape[0] # K x 1\n",
    "\n",
    "        if self.binarize is not None:\n",
    "            X = self.__binarize(X)\n",
    "\n",
    "        # Alpha will be used for smoothing later.\n",
    "        # If set to zero, we could have numerical instability.\n",
    "        if self.alpha == 0.0:\n",
    "            self.alpha = 1e-16\n",
    "\n",
    "        feature_log_probs = []\n",
    "        for k in range(len(unq)):\n",
    "            # Grab all data for the kth label\n",
    "            subset = X[y == k, :]\n",
    "\n",
    "            # We add alpha for smoothing. This means we don't take the\n",
    "            # log of zero in case a feature is missing (=> P(feature) = 0)\n",
    "            counts = np.sum(subset, axis=0) + self.alpha\n",
    "            count_sum = np.sum(counts) + self.alpha * 2\n",
    "\n",
    "            # Subtract the logs (same as division)\n",
    "            feature_log_probs.append(np.log(counts) - np.log(count_sum.reshape(-1,1)))\n",
    "\n",
    "        self._feature_log_prob = np.vstack(feature_log_probs)\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Predict the log of the label probabilities for the input\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        Returns:\n",
    "        log_probabilities -- M x K numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        if self.binarize is not None:\n",
    "            X = self.__binarize(X)\n",
    "\n",
    "        # log of the Bernoulli equation\n",
    "        neg_prob = np.log(1. - np.exp(self._feature_log_prob))\n",
    "        log_priors = np.log(self._priors)\n",
    "        return X.dot((self._feature_log_prob - neg_prob).T) + neg_prob.sum(axis=1) + log_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hyperparams_bernoulli(clf, X, y):\n",
    "    # Set the parameters by cross-validation\n",
    "    param_grid = [{'binarize': [x * 10**-2 for x in range(0, 5000)]}]\n",
    "    grid = GridSearchCV(clf, param_grid)\n",
    "    grid.fit(X, y)\n",
    "    print('done fitting')\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class METHOD:\n",
    "    gaussian, multinomial, bernoulli = range(3)\n",
    "\n",
    "method = METHOD.multinomial\n",
    "\n",
    "scores = []\n",
    "roc_auc = []\n",
    "weights = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    X_train, X_test, y_train, y_test = load_data(Train=True)\n",
    "\n",
    "    # For now, let's train only on word frequency vectors\n",
    "    X_train = X_train[:, 0:48]\n",
    "    X_test = X_test[:, 0:48]\n",
    "\n",
    "    if method == METHOD.gaussian:\n",
    "    # Gaussian Naive Bayes\n",
    "    # This doesn't really make sense here because our features aren't continuous\n",
    "    # in a way that is Gaussian, they are percentages. This might be better\n",
    "    # for things like number of capital letters.\n",
    "        clf = GaussianBayes()\n",
    "\n",
    "    elif method == METHOD.multinomial:\n",
    "        # Multinomial Naive Bayes\n",
    "        clf = MultinomialBayes()\n",
    "\n",
    "    elif method == METHOD.bernoulli:\n",
    "        # Bernoulli (multi-variate) Naive Bayes\n",
    "        # It doesn't make sense to include features that are inherently differentiated by magnitude,\n",
    "        # i.e total number of capital letters. So we should only test on word frequencies.\n",
    "        clf = BernoulliBayes(binarize=0.31) # binarize found via cross validation\n",
    "        X, y = load_data()\n",
    "        print(find_hyperparams(clf, X, y))\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    roc_auc.append(auc(fpr, tpr))\n",
    "\n",
    "if method == METHOD.gaussian:\n",
    "    weights = clf._weights\n",
    "elif method == METHOD.bernoulli or method == METHOD.multinomial:\n",
    "    weights = np.exp(clf._feature_log_prob)\n",
    "\n",
    "show_auc(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print('Accuracy. Avg: %0.5f, Std: %0.5f' % (np.mean(scores), np.std(scores)))\n",
    "print('AUC. Avg: %0.5f, Std: %0.5f' % (np.mean(roc_auc), np.std(roc_auc)))\n",
    "print('Top %d features:' % k)\n",
    "print(top_k_features(k, weights[0, :]))\n",
    "print(top_k_features(k, weights[1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
