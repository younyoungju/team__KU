{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# from soynlp.hangle import levenshtein\n",
    "# # from PreProcessing.find_common_part\n",
    "# from konlpy.tag import *\n",
    "# from PreProcessing import find_common_part\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 불러올때    \n",
    "with open('/content/drive/Shared drives/BigData/team__KU/data/result.txt', 'rb') as f:\n",
    "    result = pickle.load(f)\n",
    "\n",
    "# # 불러올때    \n",
    "# with open('data/result.txt', 'rb') as f:\n",
    "#     result = pickle.load(f)\n",
    "\n",
    "result_gensim_input = [_.split() for _ in result if _ != '']\n",
    "result_tokenizer_input = [v for i, v in enumerate(result) if i%2 == 0 and v != '']\n",
    "ebs_in_result_for_getting_max_len = [v for i, v in enumerate(result_gensim_input) if i%2 == 0 and v != []]\n",
    "ebs_in_result_gensim_input = [v for i, v in enumerate(result_gensim_input) if i%2 == 0 and v != [] ]\n",
    "google_in_result_gensim_input = [v for i, v in enumerate(result_gensim_input) if i%2 == 1 and v != []]\n",
    "    \n",
    "# Generate EBS string vectors matrix\n",
    "ws = 1\n",
    "es = 30\n",
    "\n",
    "model_cbow = Word2Vec(\n",
    "                ebs_in_result_gensim_input, \n",
    "                window = ws,\n",
    "                size =es,\n",
    "                min_count=1,\n",
    "                workers = 10\n",
    "                )\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(result_tokenizer_input)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print('단어 집합의 크기 : %d' % vocab_size)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, es))\n",
    "for w, i in t.word_index.items():\n",
    "    embedding_matrix[i] = model_cbow[w]\n",
    "print('embedding_matrix.shape :{}'.format(embedding_matrix.shape))\n",
    "\n",
    "#시퀀스 만들기\n",
    "sequences = list()\n",
    "\n",
    "for line in result_tokenizer_input: # 1,214 개의 샘플에 대해서 샘플을 1개씩 가져온다.\n",
    "    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[i-9 if i > 9 else 0:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "max_len=max(len(l) for l in sequences)\n",
    "print('샘플의 최대 길이 : {}'.format(max_len))\n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            es,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len-1,\n",
    "                            trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "# y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size = 40, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
