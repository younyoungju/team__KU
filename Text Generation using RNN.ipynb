{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from PreProcessing.Opentext import get_EBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jamo import h2j, j2hcj, j2h\n",
    "from soynlp.hangle import levenshtein, jamo_levenshtein\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer, regexp_tokenize\n",
    "from nltk.help import upenn_tagset\n",
    "from konlpy.corpus import kolaw, kobill\n",
    "from konlpy.tag import Kkma, Mecab, Okt, Komoran, Hannanum\n",
    "from PreProcessing.find_common_part import find_common_part\n",
    "from PreProcessing.Dictionary import sd, ad, kopro\n",
    "from PreProcessing.Opentext import get_EBS, get_EBS_entered, get_STT1_Google_entered, get_STT1_Google, get_STT1_Transcribe_entered\n",
    "from collections import defaultdict\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "from konlpy.tag import Kkma, Mecab, Okt, Komoran, Hannanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EBS = get_EBS(1, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE = get_STT1_Google(1, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimalnumber_to_korean(m):\n",
    "    answer = ''\n",
    "    for _ in m.group(1):\n",
    "        answer += kopro[int(_)]\n",
    "    answer += '점'\n",
    "    for _ in m.group(3):\n",
    "        answer += kopro[int(_)]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_korean(s):\n",
    "    #소수점 한국어로\n",
    "    s = re.sub(r'(\\d+)(\\.)(\\d+)', decimalnumber_to_korean, s)\n",
    "    \n",
    "    #정수 한국어로\n",
    "    fl = re.findall(r'\\d+', s)\n",
    "    fl = set(fl)\n",
    "    fl = [int(_) for _ in fl]\n",
    "    fl.sort(reverse=True)\n",
    "    fl = [str(_) for _ in fl]\n",
    "    for n in fl:\n",
    "        s = s.replace(n, kopro[int(n)])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mathsymbol_to_korean(s):\n",
    "    for ms in sd.keys():\n",
    "        if ms in s:\n",
    "            s = s.replace(ms, sd[ms])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet_to_korean(s):\n",
    "    for a in ad.keys():\n",
    "        if a in s:\n",
    "            s = s.replace(a, ad[a])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines(s):\n",
    "    \n",
    "    return re.sub(r'[\\n]+', ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_puncuations(s):\n",
    "    \n",
    "    return re.sub(r'[\\.?,‘’]+', ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dot(text, dot):\n",
    "    pattern1 = re.compile(r'(ㅂㄴㅣㄷㅏ)$')   #ㅂ 니다\n",
    "    pattern2 = re.compile(r'(.ㅔ|ㅏ|ㅓ|ㅐ|ㅗ|ㅜ)(ㅇㅛ)$') # ㅔ요, ㅏ요, ㅓ요\n",
    "#     pattern3 = re.compile(r'(ㅆ)(ㅈ|ㅊ)(ㅛ)$') \n",
    "    pattern4 = re.compile(r'(ㅂ|ㅣ)(ㄴㅣㄲㅏ)$')  #ㅂ니까\n",
    "    pattern5 = re.compile(r'(ㄴㄷㅔ)$')\n",
    "    text_list = []\n",
    "    _1 = 0\n",
    "    _2 = 0\n",
    "    _3 = 0\n",
    "    _4 = 0\n",
    "    _5 = 0\n",
    "    _6 = 0\n",
    "    for _ in text.split(' '):         \n",
    "        new_ = j2hcj(h2j(_))\n",
    "        if pattern1.findall(new_):\n",
    "            text_list.append(_.replace(_, _+dot))\n",
    "            _1 += 1\n",
    "        elif pattern2.findall(new_):\n",
    "            text_list.append(_.replace(_, _+dot))\n",
    "            _2 += 1\n",
    "            #list2.append(_)\n",
    "#         elif pattern3.findall(new_):\n",
    "#             text_list.append(_.replace(_, _+dot))\n",
    "#             _3 += 1\n",
    "#             list3.append(_)\n",
    "        elif pattern4.findall(new_):\n",
    "            text_list.append(_.replace(_, _+dot))\n",
    "            _4 += 1\n",
    "        elif pattern5.findall(new_):\n",
    "            text_list.append(_.replace(_,_+dot))\n",
    "            _5 += 1\n",
    "        else:\n",
    "            text_list.append(_)\n",
    "            _6 += 1\n",
    "            #list6.append(_)\n",
    "    print('pattern1 = {}, pattern2= {}, pattern3 = {}, pattern5 = {}, pattern6 = {}'.format(_1,_2,_3,_4,_5,_6)) \n",
    "    #print('pattern2 = ',list2)#'\\n','list5 = ',list5)\n",
    "    return text_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EBS 전처리\n",
    "def EBS_preprocessing(EBS):\n",
    "    \n",
    "#     수학기호 한국어로 변환(EBS)\n",
    "    ebs_math_conversion = mathsymbol_to_korean(EBS)\n",
    "        \n",
    "#     숫자 한국어로 변환(EBS)\n",
    "    ebs_number_conversion = number_to_korean(ebs_math_conversion)\n",
    "        \n",
    "#     알파벳 한국어로 변환(EBS)\n",
    "    ebs_alphabet_conversion = alphabet_to_korean(ebs_number_conversion)\n",
    "    \n",
    "#     특수문자 제거\n",
    "    ebs_punc_removed = remove_puncuations(ebs_alphabet_conversion)\n",
    "    \n",
    "#     점 집어넣기\n",
    "    ebs_dot_inserted = ' '.join(insert_dot(ebs_punc_removed,'.'))\n",
    "\n",
    "#     띄어쓰기 제거\n",
    "#     ebs_whitespace_removed = re.sub('[\\s]+', '', ebs_dot_inserted)\n",
    "\n",
    "    return ebs_dot_inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGLE 전처리\n",
    "def GOOGLE_preprocessing(GOOGLE):\n",
    "        \n",
    "#     개행 삭제\n",
    "    google_newline_removed = remove_newlines(GOOGLE)\n",
    "    \n",
    "#     수학기호 한국어로 변환\n",
    "    google_math_conversion = mathsymbol_to_korean(google_newline_removed)\n",
    "    \n",
    "#     숫자 한국어로 변환(GOOGLE)\n",
    "    google_number_conversion = number_to_korean(google_math_conversion)\n",
    "        \n",
    "#     알파벳 한국어로 변환(GOOGLE)\n",
    "    google_alphabet_conversion = alphabet_to_korean(google_number_conversion)\n",
    "    \n",
    "#     특수문자 제거\n",
    "    google_punc_removed = remove_puncuations(google_alphabet_conversion)\n",
    "    \n",
    "#     점 집어넣기\n",
    "    google_dot_inserted = ' '.join(insert_dot(google_punc_removed,'.'))\n",
    "    \n",
    "#     띄어쓰기 제거\n",
    "#     google_whitespace_removed = re.sub('[\\s]+', '', google_dot_inserted)\n",
    "\n",
    "    return google_dot_inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ko_pre_EBS = EBS_preprocessing(EBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_pre_GOOGLE = GOOGLE_preprocessing(GOOGLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_pre_EBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_pre_GOOGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebs_pos = kkma.pos(ko_pre_EBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_pos = kkma.pos(ko_pre_GOOGLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준희 여기까지 돌려줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_pos(pos_text):\n",
    "    alist = []\n",
    "    for _ in pos_text:\n",
    "        if _[1] in ['NNG','NNP','NNB','NR','NP', 'VV','VA','XR']:\n",
    "            alist.append(_[0])\n",
    "            continue\n",
    "    return alist\n",
    "\n",
    "# 'NNG' : 보통명사,\n",
    "# 'NNP': 고유명사,\n",
    "# 'NNB': 일반의존명사,\n",
    "# 'NR': 수사,\n",
    "# 'NP': 대명사, \n",
    "# 'VV': 동사,\n",
    "# 'VA': 형용사,\n",
    "# 'XR': 어근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ebs_pos = selected_pos(ebs_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_google_pos = selected_pos(google_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_ebs_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_google_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_into_W2V = [list_ebs_pos + list_google_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(integrated_into_W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_skip = Word2Vec(\n",
    "    integrated_into_W2V, \n",
    "    size = 100,\n",
    "    sg = 1\n",
    ")\n",
    "words_skip = list(model_skip.wv.vocab)\n",
    "print(words_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skip.wv.similar_by_word('육이구',topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EBSlist_tokens = [' '.join(Text(ma(_)).tokens) for _ in new_textE1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EBSlist_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(morphed_EBS)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print('단어 집합의 크기 : %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list()\n",
    "\n",
    "for line in morphed_EBS: # 1,214 개의 샘플에 대해서 샘플을 1개씩 가져온다.\n",
    "    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "sequences[:11] # 11개의 샘플 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word={}\n",
    "for key, value in t.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
    "    index_to_word[value] = key\n",
    "\n",
    "print('빈도수 상위 582번 단어 : {}'.format(index_to_word[582]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=max(len(l) for l in sequences)\n",
    "print('샘플의 최대 길이 : {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "print(sequences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[:3]) # 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1))\n",
    "# y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=23, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_generation(model, t, '이 병 분에 높이', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
